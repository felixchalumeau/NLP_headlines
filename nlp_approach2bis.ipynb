{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "nlp_approach1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZRX7kKfFjEV"
      },
      "source": [
        "# import libraries\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from torch.utils.data import Dataset, random_split\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import codecs"
      ],
      "id": "0ZRX7kKfFjEV",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zk4c4HGAFjEX"
      },
      "source": [
        "# set random seed and device\n",
        "\n",
        "SEED = 1\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")"
      ],
      "id": "Zk4c4HGAFjEX",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9ihJtQaFjEY"
      },
      "source": [
        "# load datasets\n",
        "\n",
        "train_df = pd.read_csv('train.csv')\n",
        "dev_df = pd.read_csv('dev.csv')"
      ],
      "id": "f9ihJtQaFjEY",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EmwQEcTNWSI"
      },
      "source": [
        "# We define our training loop\n",
        "def train(train_iter, dev_iter, model, number_epoch):\n",
        "    \"\"\"\n",
        "    Training loop for the model, which calls on eval to evaluate after each epoch\n",
        "    \"\"\"\n",
        "\n",
        "    \n",
        "    print(\"Training model.\")\n",
        "\n",
        "    for epoch in range(1, number_epoch+1):\n",
        "\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        epoch_sse = 0\n",
        "        no_observations = 0  # Observations used for training so far\n",
        "\n",
        "        for batch in train_iter:\n",
        "\n",
        "            feature, target = batch\n",
        "\n",
        "            feature, target = feature.to(device), target.to(device)\n",
        "\n",
        "            model.batch_size = target.shape[0]\n",
        "            no_observations = no_observations + target.shape[0]\n",
        "\n",
        "            predictions = model(feature).squeeze(1)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            loss = loss_fn(predictions, target)\n",
        "\n",
        "            sse, __ = model_performance(predictions.detach().cpu().numpy(), target.detach().cpu().numpy())\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()*target.shape[0]\n",
        "            epoch_sse += sse\n",
        "\n",
        "        valid_loss, valid_mse, __, __ = eval(dev_iter, model)\n",
        "\n",
        "        epoch_loss, epoch_mse = epoch_loss / no_observations, epoch_sse / no_observations\n",
        "        print(f'| Epoch: {epoch:02} | Train Loss: {epoch_loss:.2f} | Train MSE: {epoch_mse:.2f} | Train RMSE: {epoch_mse**0.5:.2f} | \\\n",
        "        Val. Loss: {valid_loss:.2f} | Val. MSE: {valid_mse:.2f} |  Val. RMSE: {valid_mse**0.5:.2f} |')"
      ],
      "id": "9EmwQEcTNWSI",
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6e85rXWNayn"
      },
      "source": [
        "# We evaluate performance on our dev set\n",
        "def eval(data_iter, model):\n",
        "    \"\"\"\n",
        "    Evaluating model performance on the dev set\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    epoch_sse = 0\n",
        "    pred_all = []\n",
        "    trg_all = []\n",
        "    no_observations = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in data_iter:\n",
        "            feature, target = batch\n",
        "\n",
        "            feature, target = feature.to(device), target.to(device)\n",
        "\n",
        "            model.batch_size = target.shape[0]\n",
        "            no_observations = no_observations + target.shape[0]\n",
        "\n",
        "            predictions = model(feature).squeeze(1)\n",
        "            loss = loss_fn(predictions, target)\n",
        "\n",
        "            # We get the mse\n",
        "            pred, trg = predictions.detach().cpu().numpy(), target.detach().cpu().numpy()\n",
        "            sse, __ = model_performance(pred, trg)\n",
        "\n",
        "            epoch_loss += loss.item()*target.shape[0]\n",
        "            epoch_sse += sse\n",
        "            pred_all.extend(pred)\n",
        "            trg_all.extend(trg)\n",
        "\n",
        "    return epoch_loss/no_observations, epoch_sse/no_observations, np.array(pred_all), np.array(trg_all)"
      ],
      "id": "u6e85rXWNayn",
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pePEqvJEFjEZ"
      },
      "source": [
        "# How we print the model performance\n",
        "def model_performance(output, target, print_output=False):\n",
        "    \"\"\"\n",
        "    Returns SSE and MSE per batch (printing the MSE and the RMSE)\n",
        "    \"\"\"\n",
        "\n",
        "    sq_error = (output - target)**2\n",
        "\n",
        "    sse = np.sum(sq_error)\n",
        "    mse = np.mean(sq_error)\n",
        "    rmse = np.sqrt(mse)\n",
        "\n",
        "    if print_output:\n",
        "        print(f'| MSE: {mse:.2f} | RMSE: {rmse:.2f} |')\n",
        "\n",
        "    return sse, mse"
      ],
      "id": "pePEqvJEFjEZ",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwO3OpF6FjEZ"
      },
      "source": [
        "def create_vocab(data):\n",
        "    \"\"\"\n",
        "    Creating a corpus of all the tokens used\n",
        "    \"\"\"\n",
        "    tokenized_corpus = [] # Let us put the tokenized corpus in a list\n",
        "\n",
        "    for sentence in data:\n",
        "\n",
        "        tokenized_sentence = []\n",
        "\n",
        "        for token in sentence.split(' '): # simplest split is\n",
        "\n",
        "            tokenized_sentence.append(token)\n",
        "\n",
        "        tokenized_corpus.append(tokenized_sentence)\n",
        "\n",
        "    # Create single list of all vocabulary\n",
        "    vocabulary = []  # Let us put all the tokens (mostly words) appearing in the vocabulary in a list\n",
        "\n",
        "    for sentence in tokenized_corpus:\n",
        "\n",
        "        for token in sentence:\n",
        "\n",
        "            if token not in vocabulary:\n",
        "\n",
        "                if True:\n",
        "                    vocabulary.append(token)\n",
        "\n",
        "    return vocabulary, tokenized_corpus"
      ],
      "id": "GwO3OpF6FjEZ",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9IzMIcHFvxM"
      },
      "source": [
        "# Approach 2"
      ],
      "id": "C9IzMIcHFvxM"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5NjwEtQFjEZ"
      },
      "source": [
        "## a. TF-IDF code (given) and baseline"
      ],
      "id": "e5NjwEtQFjEZ"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3degz_y-FjEa",
        "outputId": "9ad18900-0c6d-4fdd-e0ba-fa436189767d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train_proportion = 0.8\n",
        "train_and_dev = train_df['edit']\n",
        "\n",
        "training_data, dev_data, training_y, dev_y = train_test_split(train_df['edit'], train_df['meanGrade'],\n",
        "                                                                        test_size=(1-train_proportion),\n",
        "                                                                        random_state=42)\n",
        "\n",
        "# We train a Tf-idf model\n",
        "count_vect = CountVectorizer(stop_words='english')\n",
        "train_counts = count_vect.fit_transform(training_data)\n",
        "transformer = TfidfTransformer().fit(train_counts)\n",
        "train_counts = transformer.transform(train_counts)\n",
        "regression_model = LinearRegression().fit(train_counts, training_y)\n",
        "\n",
        "# Train predictions\n",
        "predicted_train = regression_model.predict(train_counts)\n",
        "\n",
        "# Calculate Tf-idf using train and dev, and validate model on dev:\n",
        "test_and_test_counts = count_vect.transform(train_and_dev)\n",
        "transformer = TfidfTransformer().fit(test_and_test_counts)\n",
        "\n",
        "test_counts = count_vect.transform(dev_data)\n",
        "\n",
        "test_counts = transformer.transform(test_counts)\n",
        "\n",
        "# Dev predictions\n",
        "predicted = regression_model.predict(test_counts)\n",
        "\n",
        "# We run the evaluation:\n",
        "print(\"\\nTrain performance:\")\n",
        "sse, mse = model_performance(predicted_train, training_y, True)\n",
        "\n",
        "print(\"\\nDev performance:\")\n",
        "sse, mse = model_performance(predicted, dev_y, True)"
      ],
      "id": "3degz_y-FjEa",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train performance:\n",
            "| MSE: 0.13 | RMSE: 0.37 |\n",
            "\n",
            "Dev performance:\n",
            "| MSE: 0.36 | RMSE: 0.60 |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6oS9rOoFjEa",
        "outputId": "7d7c8ffd-a9f9-46db-8370-1ea1dacbabaf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# baseline for the task\n",
        "\n",
        "pred_baseline = torch.zeros(len(dev_y)) + np.mean(training_y)\n",
        "print(\"\\nBaseline performance:\")\n",
        "sse, mse = model_performance(pred_baseline, dev_y, True)"
      ],
      "id": "Z6oS9rOoFjEa",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Baseline performance:\n",
            "| MSE: 0.34 | RMSE: 0.58 |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_qUgPYzpMPB"
      },
      "source": [
        "## b. BERT-like model\n",
        "First attempt at training a BERT-like transformer model. We thought we would create a model from scratch using the given libraries, train it on a smaller but dedicated dataset, and implement it in place of BERT. We went for a RoBERTa-like model. All elements of the model are identified with the suffix \"_2b\"\n",
        "\n",
        "Largely inspired by https://towardsdatascience.com/transformers-retraining-roberta-base-using-the-roberta-mlm-procedure-7422160d5764 "
      ],
      "id": "Q_qUgPYzpMPB"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Phgne8DdFjEf",
        "outputId": "ac0eeef7-ae61-437f-ca7a-936455f87e13",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# mount google drive to work in colab\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "!mkdir \"/content/drive/My Drive/abc/aBERTc2\""
      ],
      "id": "Phgne8DdFjEf",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "mkdir: cannot create directory ‘/content/drive/My Drive/abc/aBERTc2’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9K9E4O8ZFjEf"
      },
      "source": [
        "# extract txt from original csv dataset\n",
        "# commented out as only needed the first time\n",
        "\n",
        "# import pandas as pd\n",
        "# abcnews = pd.read_csv('/content/drive/My Drive/abc/abcnews.csv')\n",
        "\n",
        "# with open(\"/content/drive/My Drive/abc/abcheads.txt\", \"a\") as file:\n",
        "#   for line in abcnews['headline_text']:\n",
        "#     line += \"\\n\"\n",
        "#     file.write(line)"
      ],
      "id": "9K9E4O8ZFjEf",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDRIbRlDFjEf",
        "outputId": "5d349851-369f-42de-dfeb-2c459474ce8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        }
      },
      "source": [
        "# import premade tokenizer (byte-level BPE)\n",
        "\n",
        "from transformers import RobertaTokenizer\n",
        "\n",
        "tokenizer_2b = RobertaTokenizer.from_pretrained('roberta-base')"
      ],
      "id": "jDRIbRlDFjEf",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-36ecfd2dee34>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# import premade tokenizer (byte-level BPE)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRobertaTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtokenizer_2b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRobertaTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'roberta-base'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1K6SeeQFjEg"
      },
      "source": [
        "# prepare txt dataset with tokenizer\n",
        "\n",
        "from transformers import LineByLineTextDataset\n",
        "\n",
        "dataset_2b = LineByLineTextDataset(\n",
        "    tokenizer=tokenizer_2b,\n",
        "    file_path=\"/content/drive/My Drive/abc/abcheads.txt\",\n",
        "    block_size=128,\n",
        ")"
      ],
      "id": "b1K6SeeQFjEg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vZUTcKIFjEg"
      },
      "source": [
        "# prepare data collator (speeds up things, not sure what)\n",
        "\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "data_collator_2b = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer_2b, mlm=True, mlm_probability=0.15\n",
        ")"
      ],
      "id": "4vZUTcKIFjEg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjdHW3CVFjEg"
      },
      "source": [
        "# create Roberta model from scratch, just prepare config\n",
        "\n",
        "from transformers import RobertaConfig, RobertaForMaskedLM\n",
        "\n",
        "config_2b = RobertaConfig(\n",
        "    vocab_size=52_000,\n",
        "    max_position_embeddings=514,\n",
        "    num_attention_heads=12,\n",
        "    num_hidden_layers=6,\n",
        "    type_vocab_size=1,\n",
        ")\n",
        "\n",
        "model_2b = RobertaForMaskedLM(config=config_2b)"
      ],
      "id": "XjdHW3CVFjEg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whg0C5JvFjEh"
      },
      "source": [
        "# set up training configurations\n",
        "\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args_2b = TrainingArguments(\n",
        "    output_dir=\"/content/drive/My Drive/abc/aBERTc2\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=64,\n",
        "    save_steps=10_000,\n",
        "    save_total_limit=2,\n",
        "    seed=1\n",
        ")\n",
        "\n",
        "trainer_2b = Trainer(\n",
        "    model=model_2b,\n",
        "    args=training_args_2b,\n",
        "    data_collator=data_collator_2b,\n",
        "    train_dataset=dataset_2b\n",
        ")"
      ],
      "id": "whg0C5JvFjEh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g632xY6AFjEh"
      },
      "source": [
        "# verify model parameters ~ 84M\n",
        "\n",
        "model_2b.num_parameters()"
      ],
      "id": "g632xY6AFjEh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYZlmOT1FjEi"
      },
      "source": [
        "# train and save trained model\n",
        "\n",
        "trainer_2b.train()\n",
        "trainer_2b.save_model(\"/content/drive/My Drive/abc/aBERTc2\")"
      ],
      "id": "tYZlmOT1FjEi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cccb05fSFjEj"
      },
      "source": [
        "# sanity check on model performance\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "fill_mask = pipeline(\n",
        "    \"fill-mask\",\n",
        "    model=\"/content/drive/My Drive/abc/aBERTc2\",\n",
        "    tokenizer=\"roberta-base\"\n",
        ")\n",
        "fill_mask(\"Send these <mask> back!\")"
      ],
      "id": "Cccb05fSFjEj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "va8hUPFeFjEk"
      },
      "source": [
        "At this point we tried replacing the newly trained RoBERTa model in the pipeline we had designed for Approach 1.\n",
        "\n",
        "(Follows Samy's code and explanation of what didn't work)"
      ],
      "id": "va8hUPFeFjEk"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkX8TYapFjEk"
      },
      "source": [
        "## c. FFN with word embeddings\n",
        "As second attempt we tried a modified version of a 2-layered feed-forward neural networks, modified from the 2nd lab of the course. We substituted the classification layer at the top with another linear layer in order to make it appropriate for the regression. We used all the words from the original headline + the edited word, stripped of its punctuation."
      ],
      "id": "pkX8TYapFjEk"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saoHkr4sFjEk"
      },
      "source": [
        "# recreate tok_corpus, word2idx and related functions, with punctuation removal\n",
        "\n",
        "import re\n",
        "re_punctuation_string = '[\\s,/.\\'<>]'\n",
        "\n",
        "def get_tokenized_corpus(corpus):\n",
        "  tokenized_corpus = []\n",
        "\n",
        "  for sentence in corpus:\n",
        "    tokenized_sentence = []\n",
        "    for token in re.split(' ', sentence): \n",
        "      tokenized_sentence.append(token)\n",
        "    tokenized_corpus.append(tokenized_sentence)\n",
        " \n",
        "  return tokenized_corpus\n",
        "\n",
        "\n",
        "def get_word2idx(tokenized_corpus):\n",
        "  vocabulary = []\n",
        "  for sentence in tokenized_corpus:\n",
        "    for token in sentence:\n",
        "        if token not in vocabulary:\n",
        "            vocabulary.append(token)\n",
        "  \n",
        "  word2idx = {w: idx+1 for (idx, w) in enumerate(vocabulary)}\n",
        "  word2idx['<pad>'] = 0\n",
        "  \n",
        "  return word2idx\n",
        "\n",
        "\n",
        "def get_model_inputs(tokenized_corpus, word2idx, labels):\n",
        "  vectorized_sents = [[word2idx[tok] for tok in sent if tok in word2idx] for sent in tokenized_corpus]\n",
        "  sent_lengths = [len(sent) for sent in vectorized_sents]\n",
        "  max_len = max(sent_lengths)\n",
        "  sent_tensor = torch.zeros((len(vectorized_sents), max_len)).long()\n",
        "  for idx, (sent, sentlen) in enumerate(zip(vectorized_sents, sent_lengths)):\n",
        "    sent_tensor[idx, :sentlen] = torch.LongTensor(sent)\n",
        "  label_tensor = torch.FloatTensor(labels)\n",
        "  return sent_tensor, label_tensor"
      ],
      "id": "saoHkr4sFjEk",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cEmmvFtFjEl"
      },
      "source": [
        "# construct network model\n",
        "\n",
        "class FFNN_2c(nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_dim, vocab_size):  \n",
        "        super(FFNN_2c, self).__init__()\n",
        "\n",
        "        # padding_idx argument makes sure that the 0-th token in the vocabulary\n",
        "        # is used for padding purposes i.e. its embedding will be a 0-vector\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.fc1 = nn.Linear(embedding_dim, hidden_dim)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_dim, 1)\n",
        "        self.relu2 = nn.ReLU()  \n",
        "    \n",
        "    def forward(self, x):\n",
        "        # x has shape (batch_size, max_sent_len)\n",
        "\n",
        "        embedded = self.embedding(x)\n",
        "        sent_lens = x.ne(0).sum(1, keepdims=True)\n",
        "        averaged = embedded.sum(1) / sent_lens\n",
        "\n",
        "        out = self.fc1(averaged)\n",
        "        out = self.relu1(out)\n",
        "        out = self.fc2(out)\n",
        "        out = self.relu2(out)\n",
        "        out = torch.clamp(out, max=3.0)\n",
        "        return out"
      ],
      "id": "9cEmmvFtFjEl",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUZOd_8BFjEm"
      },
      "source": [
        "# prepare traning set\n",
        "\n",
        "tokenized_corpus = get_tokenized_corpus(train_df[\"original\"].str.cat(train_df[\"edit\"], sep=\" \").tolist())\n",
        "word2idx = get_word2idx(tokenized_corpus)\n",
        "train_sent_tensor, train_label_tensor = get_model_inputs(tokenized_corpus, word2idx, train_df['meanGrade'])"
      ],
      "id": "KUZOd_8BFjEm",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQXR8zwmFjEm",
        "outputId": "d367519b-7325-46b2-fd50-798118ecba42",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# run model on training dataset\n",
        "\n",
        "EPOCHS = 10\n",
        "LRATE = 0.5\n",
        "EMBEDDING_DIM = 80\n",
        "HIDDEN_DIM = 80\n",
        "\n",
        "model_2c = FFNN_2c(EMBEDDING_DIM, HIDDEN_DIM, len(word2idx))\n",
        "optimizer_2c = torch.optim.SGD(model_2c.parameters(), lr=LRATE)\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "feature = train_sent_tensor\n",
        "target = train_label_tensor\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "  model_2c.train()\n",
        "  optimizer_2c.zero_grad()\n",
        "  \n",
        "  predictions = model_2c(feature).squeeze(1)\n",
        "  loss = loss_fn(predictions, target)\n",
        "  train_loss = loss.item()\n",
        "  \n",
        "  loss.backward()\n",
        "  optimizer_2c.step()\n",
        "  \n",
        "  print(f'| Epoch: {epoch:02} | Train Loss: {train_loss:.3f}')"
      ],
      "id": "kQXR8zwmFjEm",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| Epoch: 01 | Train Loss: 1.205\n",
            "| Epoch: 02 | Train Loss: 0.786\n",
            "| Epoch: 03 | Train Loss: 0.580\n",
            "| Epoch: 04 | Train Loss: 0.544\n",
            "| Epoch: 05 | Train Loss: 0.398\n",
            "| Epoch: 06 | Train Loss: 0.367\n",
            "| Epoch: 07 | Train Loss: 0.351\n",
            "| Epoch: 08 | Train Loss: 0.347\n",
            "| Epoch: 09 | Train Loss: 0.345\n",
            "| Epoch: 10 | Train Loss: 0.344\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ga7Kuj4_FjEm"
      },
      "source": [
        "# prepare validation dataset\n",
        "\n",
        "dev_tokenized_corpus = get_tokenized_corpus(dev_df[\"original\"].str.cat(dev_df[\"edit\"], sep=\" \").tolist())\n",
        "dev_sent_tensor, dev_label_tensor = get_model_inputs(dev_tokenized_corpus, word2idx, dev_df['meanGrade'])"
      ],
      "id": "ga7Kuj4_FjEm",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GC3m5XI9FjEm",
        "outputId": "2d65cbe0-b01a-4292-9dde-0bba1c4e448a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# optimise hyperparameters on validation set\n",
        "\n",
        "EPOCHS = 30\n",
        "LRATE = 0.5\n",
        "EMBEDDING_DIM = 50\n",
        "HIDDEN_DIM = 50\n",
        "OUTPUT_DIM = 1\n",
        "\n",
        "model_2c = FFNN_2c(EMBEDDING_DIM, HIDDEN_DIM, len(word2idx))\n",
        "optimizer_2c = torch.optim.SGD(model_2c.parameters(), lr=LRATE)\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "feature_train = train_sent_tensor\n",
        "target_train = train_label_tensor\n",
        "\n",
        "feature_valid = dev_sent_tensor\n",
        "target_valid = dev_label_tensor\n",
        "\n",
        "print(f'Will train for {EPOCHS} epochs')\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "  model_2c.train()\n",
        "  optimizer_2c.zero_grad()\n",
        "  predictions = model_2c(feature_train).squeeze(1)\n",
        "\n",
        "  loss = loss_fn(predictions, target_train)\n",
        "  train_loss = loss.item()\n",
        "\n",
        "  loss.backward()\n",
        "  optimizer_2c.step()\n",
        "  \n",
        "  model_2c.eval()\n",
        "  with torch.no_grad():\n",
        "    predictions_valid = model_2c(feature_valid).squeeze(1)\n",
        "    valid_loss = loss_fn(predictions_valid, target_valid).item()\n",
        "  \n",
        "  print(f'| Epoch: {epoch:02} | Train Loss: {train_loss:.3f} | Val. Loss: {valid_loss:.3f} |')"
      ],
      "id": "GC3m5XI9FjEm",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Will train for 30 epochs\n",
            "| Epoch: 01 | Train Loss: 1.216 | Val. Loss: 1.208 |\n",
            "| Epoch: 02 | Train Loss: 1.216 | Val. Loss: 1.208 |\n",
            "| Epoch: 03 | Train Loss: 1.216 | Val. Loss: 1.208 |\n",
            "| Epoch: 04 | Train Loss: 1.216 | Val. Loss: 1.208 |\n",
            "| Epoch: 05 | Train Loss: 1.216 | Val. Loss: 1.208 |\n",
            "| Epoch: 06 | Train Loss: 1.216 | Val. Loss: 1.208 |\n",
            "| Epoch: 07 | Train Loss: 1.216 | Val. Loss: 1.208 |\n",
            "| Epoch: 08 | Train Loss: 1.216 | Val. Loss: 1.208 |\n",
            "| Epoch: 09 | Train Loss: 1.216 | Val. Loss: 1.208 |\n",
            "| Epoch: 10 | Train Loss: 1.216 | Val. Loss: 1.208 |\n",
            "| Epoch: 11 | Train Loss: 1.216 | Val. Loss: 1.208 |\n",
            "| Epoch: 12 | Train Loss: 1.216 | Val. Loss: 1.208 |\n",
            "| Epoch: 13 | Train Loss: 1.216 | Val. Loss: 1.208 |\n",
            "| Epoch: 14 | Train Loss: 1.216 | Val. Loss: 1.208 |\n",
            "| Epoch: 15 | Train Loss: 1.216 | Val. Loss: 1.208 |\n",
            "| Epoch: 16 | Train Loss: 1.216 | Val. Loss: 1.208 |\n",
            "| Epoch: 17 | Train Loss: 1.216 | Val. Loss: 1.208 |\n",
            "| Epoch: 18 | Train Loss: 1.216 | Val. Loss: 1.208 |\n",
            "| Epoch: 19 | Train Loss: 1.216 | Val. Loss: 1.208 |\n",
            "| Epoch: 20 | Train Loss: 1.216 | Val. Loss: 1.208 |\n",
            "| Epoch: 21 | Train Loss: 1.216 | Val. Loss: 1.208 |\n",
            "| Epoch: 22 | Train Loss: 1.216 | Val. Loss: 1.208 |\n",
            "| Epoch: 23 | Train Loss: 1.216 | Val. Loss: 1.208 |\n",
            "| Epoch: 24 | Train Loss: 1.216 | Val. Loss: 1.208 |\n",
            "| Epoch: 25 | Train Loss: 1.216 | Val. Loss: 1.208 |\n",
            "| Epoch: 26 | Train Loss: 1.216 | Val. Loss: 1.208 |\n",
            "| Epoch: 27 | Train Loss: 1.216 | Val. Loss: 1.208 |\n",
            "| Epoch: 28 | Train Loss: 1.216 | Val. Loss: 1.208 |\n",
            "| Epoch: 29 | Train Loss: 1.216 | Val. Loss: 1.208 |\n",
            "| Epoch: 30 | Train Loss: 1.216 | Val. Loss: 1.208 |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSlgWSTJFjEn"
      },
      "source": [
        "## d. Freestyle approach with text-based engineered features"
      ],
      "id": "RSlgWSTJFjEn"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9FpmYiuFjEn"
      },
      "source": [
        "punctuations = '''!()-[]{};:‘’\"\\,.?@#$%^&*_~'''\n",
        "sex_words = [\"orgy\", \"dick\", \"suck\", \"underwear\", \"sex\", \"sexual\", \"massage\", \"flirt\", \"kiss\", \"ass\", \"penis\"]\n",
        "\n",
        "def create_engineered_features(train_df, punctuations, sex_words):\n",
        "    train_df[\"lower_case\"] = train_df.apply(\n",
        "            lambda x: x[\"original\"].lower(), axis=1\n",
        "        )\n",
        "    train_df[\"edit_lower\"] = train_df.apply(\n",
        "            lambda x: x[\"edit\"].lower(), axis=1\n",
        "        )\n",
        "    train_df[\"nb_words\"] = train_df.apply(\n",
        "            lambda x: len(x[\"lower_case\"].split(\" \")), axis=1\n",
        "        )\n",
        "    train_df[\"nb_caracters\"] = train_df.apply(\n",
        "            lambda x: len(x[\"lower_case\"]), axis=1\n",
        "        )\n",
        "    train_df[\"nb_ponctuations\"] = train_df.apply(\n",
        "            lambda x: sum([1 if char in punctuations else 0 for char in x[\"lower_case\"]]), axis=1\n",
        "        )\n",
        "    train_df[\"edit_position\"] = train_df.apply(\n",
        "            lambda x: x[\"lower_case\"].find(\"<\"), axis=1\n",
        "        )\n",
        "    train_df[\"edit_rel_position\"] = train_df.apply(\n",
        "            lambda x: x[\"edit_position\"]/x[\"nb_caracters\"], axis=1\n",
        "        )\n",
        "    train_df[\"len_edit\"] = train_df.apply(\n",
        "            lambda x: len(x[\"edit_lower\"]), axis=1\n",
        "        )\n",
        "    # i need trump\n",
        "    train_df[\"trump_in_original\"] = train_df.apply(\n",
        "            lambda x: float(x[\"lower_case\"].find(\"trump\") != -1), axis=1\n",
        "        )\n",
        "    train_df[\"trump_in_edit\"] = train_df.apply(\n",
        "            lambda x: float(x[\"edit_lower\"].find(\"trump\") != -1), axis=1\n",
        "        )\n",
        "\n",
        "    # i need hair\n",
        "    train_df[\"hair_in_original\"] = train_df.apply(\n",
        "            lambda x: float(x[\"lower_case\"].find(\"hair\") != -1), axis=1\n",
        "        )\n",
        "    train_df[\"hair_in_edit\"] = train_df.apply(\n",
        "            lambda x: float(x[\"edit_lower\"].find(\"hair\") != -1), axis=1\n",
        "        )\n",
        "    train_df[\"sex_in_edit\"] = train_df.apply(\n",
        "            lambda x: float(x[\"edit_lower\"] in sex_words), axis=1\n",
        "        )\n",
        "\n",
        "    return train_df"
      ],
      "id": "P9FpmYiuFjEn",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Za6tRgxHFjEn",
        "outputId": "d6dee01f-4aee-4f94-fde4-31ec20ef244e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        }
      },
      "source": [
        "train_df = create_engineered_features(train_df, punctuations, sex_words)\n",
        "train_df.head()"
      ],
      "id": "Za6tRgxHFjEn",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>original</th>\n",
              "      <th>edit</th>\n",
              "      <th>grades</th>\n",
              "      <th>meanGrade</th>\n",
              "      <th>lower_case</th>\n",
              "      <th>edit_lower</th>\n",
              "      <th>nb_words</th>\n",
              "      <th>nb_caracters</th>\n",
              "      <th>nb_ponctuations</th>\n",
              "      <th>edit_position</th>\n",
              "      <th>edit_rel_position</th>\n",
              "      <th>len_edit</th>\n",
              "      <th>trump_in_original</th>\n",
              "      <th>trump_in_edit</th>\n",
              "      <th>hair_in_original</th>\n",
              "      <th>hair_in_edit</th>\n",
              "      <th>sex_in_edit</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>14530</td>\n",
              "      <td>France is ‘ hunting down its citizens who join...</td>\n",
              "      <td>twins</td>\n",
              "      <td>10000</td>\n",
              "      <td>0.2</td>\n",
              "      <td>france is ‘ hunting down its citizens who join...</td>\n",
              "      <td>twins</td>\n",
              "      <td>15</td>\n",
              "      <td>80</td>\n",
              "      <td>2</td>\n",
              "      <td>49</td>\n",
              "      <td>0.612500</td>\n",
              "      <td>5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>13034</td>\n",
              "      <td>Pentagon claims 2,000 % increase in Russian tr...</td>\n",
              "      <td>bowling</td>\n",
              "      <td>33110</td>\n",
              "      <td>1.6</td>\n",
              "      <td>pentagon claims 2,000 % increase in russian tr...</td>\n",
              "      <td>bowling</td>\n",
              "      <td>17</td>\n",
              "      <td>97</td>\n",
              "      <td>4</td>\n",
              "      <td>57</td>\n",
              "      <td>0.587629</td>\n",
              "      <td>7</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8731</td>\n",
              "      <td>Iceland PM Calls Snap Vote as Pedophile Furor ...</td>\n",
              "      <td>party</td>\n",
              "      <td>22100</td>\n",
              "      <td>1.0</td>\n",
              "      <td>iceland pm calls snap vote as pedophile furor ...</td>\n",
              "      <td>party</td>\n",
              "      <td>11</td>\n",
              "      <td>67</td>\n",
              "      <td>0</td>\n",
              "      <td>54</td>\n",
              "      <td>0.805970</td>\n",
              "      <td>5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>76</td>\n",
              "      <td>In an apparent first , Iran and Israel &lt;engage...</td>\n",
              "      <td>slap</td>\n",
              "      <td>20000</td>\n",
              "      <td>0.4</td>\n",
              "      <td>in an apparent first , iran and israel &lt;engage...</td>\n",
              "      <td>slap</td>\n",
              "      <td>12</td>\n",
              "      <td>70</td>\n",
              "      <td>1</td>\n",
              "      <td>39</td>\n",
              "      <td>0.557143</td>\n",
              "      <td>4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6164</td>\n",
              "      <td>Trump was told weeks ago that Flynn misled &lt;Vi...</td>\n",
              "      <td>school</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>trump was told weeks ago that flynn misled &lt;vi...</td>\n",
              "      <td>school</td>\n",
              "      <td>11</td>\n",
              "      <td>62</td>\n",
              "      <td>1</td>\n",
              "      <td>43</td>\n",
              "      <td>0.693548</td>\n",
              "      <td>6</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      id  ... sex_in_edit\n",
              "0  14530  ...         0.0\n",
              "1  13034  ...         0.0\n",
              "2   8731  ...         0.0\n",
              "3     76  ...         0.0\n",
              "4   6164  ...         0.0\n",
              "\n",
              "[5 rows x 18 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6XUANDCFjEo"
      },
      "source": [
        "# extract labels and features from training set \n",
        "\n",
        "columns = [\n",
        "    \"nb_words\",\n",
        "    \"nb_caracters\",\n",
        "    \"nb_ponctuations\",\n",
        "    \"edit_position\",\n",
        "    \"edit_rel_position\",\n",
        "    \"len_edit\",\t\"trump_in_original\",\n",
        "    \"trump_in_edit\",\n",
        "    \"hair_in_original\",\n",
        "    \"hair_in_edit\",\n",
        "    \"sex_in_edit\"\n",
        "    ]\n",
        "    \n",
        "features = train_df[columns].values\n",
        "labels = train_df.meanGrade.values"
      ],
      "id": "z6XUANDCFjEo",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wl6cHhEUFjEo"
      },
      "source": [
        "# prepare the validation set\n",
        "\n",
        "dev_df = pd.read_csv('dev.csv')\n",
        "dev_df = create_engineered_features(dev_df, punctuations, sex_words)\n",
        "dev_inputs = dev_df[columns].values\n",
        "dev_labels = dev_df['meanGrade'].values"
      ],
      "id": "Wl6cHhEUFjEo",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCpp4UWUFjEo",
        "outputId": "ac437316-9a5f-4049-d8f8-1f62d84608f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# create dataloaders\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_inputs = torch.tensor(features, dtype=torch.float32)\n",
        "train_labels = torch.tensor(labels, dtype=torch.float32)\n",
        "\n",
        "validation_inputs = torch.tensor(dev_inputs, dtype=torch.float32)\n",
        "validation_labels = torch.tensor(dev_labels, dtype=torch.float32)\n",
        "\n",
        "train_data = torch.utils.data.TensorDataset(train_inputs, train_labels)\n",
        "dev_data = torch.utils.data.TensorDataset(validation_inputs, validation_labels)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_data, shuffle=True, batch_size=BATCH_SIZE)\n",
        "validation_loader = torch.utils.data.DataLoader(dev_data, shuffle=False, batch_size=BATCH_SIZE)\n",
        "\n",
        "print(\"Dataloaders created.\")"
      ],
      "id": "NCpp4UWUFjEo",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataloaders created.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qR34WS45FjEo"
      },
      "source": [
        "# design model architecture in torch\n",
        "\n",
        "class FFNN_2d(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(FFNN_2d, self).__init__()\n",
        "\n",
        "        self.fc1 = torch.nn.Linear(11, 10)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.fc2 = torch.nn.Linear(10, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        out = self.fc1(x)\n",
        "        out = self.relu1(out)\n",
        "        out = self.fc2(out)\n",
        "        out = torch.clamp(out, min=0.0, max=3.0)\n",
        "        return out"
      ],
      "id": "qR34WS45FjEo",
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAQsjMMHFjEp"
      },
      "source": [
        "# set hyperparameters, model, optimizer and loss function\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "num_epochs = 2 #2\n",
        "learning_rate = 1.8e-6 #2e-6\n",
        "adam_eps = 1e-8\n",
        "\n",
        "model_2d = FFNN_2d()\n",
        "model_2d.to(device)\n",
        "model_2d.train()\n",
        "optimizer = optim.Adam(model_2d.parameters(), lr=learning_rate, eps=adam_eps)\n",
        "\n",
        "loss_fn = nn.MSELoss()\n",
        "loss_fn = loss_fn.to(device)"
      ],
      "id": "mAQsjMMHFjEp",
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wl34kY-wFjEp",
        "outputId": "e5a50993-7fb6-4bbe-9b5e-56157eb7a9c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# train model\n",
        "\n",
        "train(train_loader, validation_loader, model_2d, num_epochs)"
      ],
      "id": "Wl34kY-wFjEp",
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training model.\n",
            "| Epoch: 01 | Train Loss: 1.22 | Train MSE: 1.22 | Train RMSE: 1.10 |         Val. Loss: 1.21 | Val. MSE: 1.21 |  Val. RMSE: 1.10 |\n",
            "| Epoch: 02 | Train Loss: 1.22 | Train MSE: 1.22 | Train RMSE: 1.10 |         Val. Loss: 1.21 | Val. MSE: 1.21 |  Val. RMSE: 1.10 |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K--yw-oAFjEt",
        "outputId": "6898279a-70a8-4c92-e134-cc98f7b15faa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "source": [
        "# make predictions\n",
        "train_predictions = []\n",
        "model_perf = 0\n",
        "mean_perf = 0\n",
        "true_labels = []\n",
        "with torch.no_grad():\n",
        "    for batch in train_loader:\n",
        "        # add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        # unzip\n",
        "        input_ids, targets = batch\n",
        "        # predict (forward pass)\n",
        "        pred = model_2d(input_ids)\n",
        "        train_predictions += [pred]\n",
        "        true_labels += [targets]\n",
        "\n",
        "        mean_predictions = torch.ones_like(pred) * mean_value\n",
        "        mean_predictions = mean_predictions.to(device)\n",
        "\n",
        "        # for the model\n",
        "        # get scores\n",
        "        pred = pred.squeeze(1)\n",
        "        train_loss = loss_fn(pred, targets)\n",
        "\n",
        "        # We get the mse\n",
        "        pred, trg = pred.detach().cpu().numpy(), targets.detach().cpu().numpy()\n",
        "        sse, __ = model_performance(pred, trg)\n",
        "        model_perf += sse\n",
        "\n",
        "        # for the mean prediction\n",
        "        \n",
        "        # get scores\n",
        "        mean_predictions = mean_predictions.squeeze(1)\n",
        "        mean_loss = loss_fn(mean_predictions, targets)\n",
        "\n",
        "        # We get the mse\n",
        "        pred_mean, trg_mean = mean_predictions.detach().cpu().numpy(), targets.detach().cpu().numpy()\n",
        "        sse_mean, __ = model_performance(pred_mean, trg_mean)\n",
        "        mean_perf += sse_mean\n",
        "\n",
        "\n",
        "train_predictions = torch.cat(train_predictions, dim=0)\n",
        "true_labels = torch.cat(true_labels, dim=0).unsqueeze(1)\n",
        "check_values = torch.cat((train_predictions, true_labels), dim=1)\n",
        "print(\"Performances : --- Model : {} --- --- Mean predictor : {} ---\".format(model_perf, mean_perf))\n",
        "print(\"Values : \", check_values)"
      ],
      "id": "K--yw-oAFjEt",
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-c8e59318315a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mtrue_labels\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mmean_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmean_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mmean_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_predictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'mean_value' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x71R-0fLFjEt",
        "outputId": "d6f358f6-6f13-4173-de50-4ee13385f224",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# make predictions\n",
        "\n",
        "mean_value = 0.9355712114933001\n",
        "validation_predictions = []\n",
        "model_perf = 0\n",
        "mean_perf = 0\n",
        "true_labels = []\n",
        "with torch.no_grad():\n",
        "    for batch in validation_loader:\n",
        "        # add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        # unzip\n",
        "        input_ids, targets = batch\n",
        "        # predict (forward pass)\n",
        "        pred = model_2d(input_ids)\n",
        "        validation_predictions += [pred]\n",
        "        true_labels += [targets]\n",
        "\n",
        "        mean_predictions = torch.ones_like(pred) * mean_value\n",
        "        mean_predictions = mean_predictions.to(device)\n",
        "\n",
        "        # for the model\n",
        "        # get scores\n",
        "        pred = pred.squeeze(1)\n",
        "        validation_loss = loss_fn(pred, targets)\n",
        "\n",
        "        # We get the mse\n",
        "        pred, trg = pred.detach().cpu().numpy(), targets.detach().cpu().numpy()\n",
        "        sse, __ = model_performance(pred, trg)\n",
        "        model_perf += sse\n",
        "\n",
        "        # for the mean prediction\n",
        "        \n",
        "        # get scores\n",
        "        mean_predictions = mean_predictions.squeeze(1)\n",
        "        mean_loss = loss_fn(mean_predictions, targets)\n",
        "\n",
        "        # We get the mse\n",
        "        pred_mean, trg_mean = mean_predictions.detach().cpu().numpy(), targets.detach().cpu().numpy()\n",
        "        sse_mean, __ = model_performance(pred_mean, trg_mean)\n",
        "        mean_perf += sse_mean\n",
        "\n",
        "\n",
        "validation_predictions = torch.cat(validation_predictions, dim=0)\n",
        "true_labels = torch.cat(true_labels, dim=0).unsqueeze(1)\n",
        "check_values = torch.cat((validation_predictions, true_labels), dim=1)\n",
        "print(\"Performances : --- Model : {} --- --- Mean predictor : {} ---\".format(model_perf, mean_perf))\n",
        "print(\"Values : \", check_values)"
      ],
      "id": "x71R-0fLFjEt",
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Performances : --- Model : 2922.9600105285645 --- --- Mean predictor : 809.26771068573 ---\n",
            "Values :  tensor([[0.0000, 1.0000],\n",
            "        [0.0000, 0.8000],\n",
            "        [0.0000, 0.6000],\n",
            "        ...,\n",
            "        [0.0000, 1.4000],\n",
            "        [0.0000, 1.4000],\n",
            "        [0.0000, 0.6000]], device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3HjO9WbFjEu",
        "outputId": "55659727-065e-4d1c-ae29-683e5b54d196",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        }
      },
      "source": [
        "# get the data\n",
        "test_df = pd.read_csv('test.csv')\n",
        "# transforms the headlines\n",
        "test_df = create_engineered_features(test_df, punctuations, sex_words)\n",
        "test_input_ids = test_df[columns].values\n",
        "\n",
        "# convert to tensor\n",
        "test_inputs = torch.tensor(test_input_ids)\n",
        "\n",
        "# create \n",
        "test_data = torch.utils.data.TensorDataset(test_inputs)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, shuffle=False, batch_size=BATCH_SIZE)\n",
        "\n",
        "# make predictions\n",
        "test_predictions = []\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        # add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        # unzip\n",
        "        input_ids = batch\n",
        "        # predict (forward pass)\n",
        "        pred = model_2d(input_ids)\n",
        "        test_predictions += [pred]\n",
        "\n",
        "test_predictions = torch.cat(test_predictions, dim=0)\n",
        "test_predictions"
      ],
      "id": "A3HjO9WbFjEu",
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-469f8af027ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# get the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# transforms the headlines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_engineered_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpunctuations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msex_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtest_input_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'test.csv'"
          ]
        }
      ]
    }
  ]
}