{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "nlp_approach1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import libraries\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from torch.utils.data import Dataset, random_split\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import codecs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# set random seed and device\n",
        "\n",
        "SEED = 1\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load datasets\n",
        "\n",
        "train_df = pd.read_csv('train.csv')\n",
        "dev_df = pd.read_csv('dev.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# How we print the model performance\n",
        "def model_performance(output, target, print_output=False):\n",
        "    \"\"\"\n",
        "    Returns SSE and MSE per batch (printing the MSE and the RMSE)\n",
        "    \"\"\"\n",
        "\n",
        "    sq_error = (output - target)**2\n",
        "\n",
        "    sse = np.sum(sq_error)\n",
        "    mse = np.mean(sq_error)\n",
        "    rmse = np.sqrt(mse)\n",
        "\n",
        "    if print_output:\n",
        "        print(f'| MSE: {mse:.2f} | RMSE: {rmse:.2f} |')\n",
        "\n",
        "    return sse, mse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_vocab(data):\n",
        "    \"\"\"\n",
        "    Creating a corpus of all the tokens used\n",
        "    \"\"\"\n",
        "    tokenized_corpus = [] # Let us put the tokenized corpus in a list\n",
        "\n",
        "    for sentence in data:\n",
        "\n",
        "        tokenized_sentence = []\n",
        "\n",
        "        for token in sentence.split(' '): # simplest split is\n",
        "\n",
        "            tokenized_sentence.append(token)\n",
        "\n",
        "        tokenized_corpus.append(tokenized_sentence)\n",
        "\n",
        "    # Create single list of all vocabulary\n",
        "    vocabulary = []  # Let us put all the tokens (mostly words) appearing in the vocabulary in a list\n",
        "\n",
        "    for sentence in tokenized_corpus:\n",
        "\n",
        "        for token in sentence:\n",
        "\n",
        "            if token not in vocabulary:\n",
        "\n",
        "                if True:\n",
        "                    vocabulary.append(token)\n",
        "\n",
        "    return vocabulary, tokenized_corpus"
      ]
    },
    {
      "source": [
        "# Approach 2\n",
        "## a. Given code and baseline"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_and_dev = train_df['edit']\n",
        "\n",
        "training_data, dev_data, training_y, dev_y = train_test_split(train_df['edit'], train_df['meanGrade'],\n",
        "                                                                        test_size=(1-train_proportion),\n",
        "                                                                        random_state=42)\n",
        "\n",
        "# We train a Tf-idf model\n",
        "count_vect = CountVectorizer(stop_words='english')\n",
        "train_counts = count_vect.fit_transform(training_data)\n",
        "transformer = TfidfTransformer().fit(train_counts)\n",
        "train_counts = transformer.transform(train_counts)\n",
        "regression_model = LinearRegression().fit(train_counts, training_y)\n",
        "\n",
        "# Train predictions\n",
        "predicted_train = regression_model.predict(train_counts)\n",
        "\n",
        "# Calculate Tf-idf using train and dev, and validate model on dev:\n",
        "test_and_test_counts = count_vect.transform(train_and_dev)\n",
        "transformer = TfidfTransformer().fit(test_and_test_counts)\n",
        "\n",
        "test_counts = count_vect.transform(dev_data)\n",
        "\n",
        "test_counts = transformer.transform(test_counts)\n",
        "\n",
        "# Dev predictions\n",
        "predicted = regression_model.predict(test_counts)\n",
        "\n",
        "# We run the evaluation:\n",
        "print(\"\\nTrain performance:\")\n",
        "sse, mse = model_performance(predicted_train, training_y, True)\n",
        "\n",
        "print(\"\\nDev performance:\")\n",
        "sse, mse = model_performance(predicted, dev_y, True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Â baseline for the task\n",
        "\n",
        "pred_baseline = torch.zeros(len(dev_y)) + np.mean(training_y)\n",
        "print(\"\\nBaseline performance:\")\n",
        "sse, mse = model_performance(pred_baseline, dev_y, True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_qUgPYzpMPB"
      },
      "source": [
        "## b. BERT-like model\n",
        "First attempt at training a BERT-like transformer model. We thought we would create a model from scratch using the given libraries, train it on a smaller but dedicated dataset, and implement it in place of BERT. We went for a RoBERTa-like model. All elements of the model are identified with the suffix \"_2b\"\n",
        "\n",
        "Largely inspired by https://towardsdatascience.com/transformers-retraining-roberta-base-using-the-roberta-mlm-procedure-7422160d5764 "
      ],
      "id": "Q_qUgPYzpMPB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# mount google drive to work in colab\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "!mkdir \"/content/drive/My Drive/abc/aBERTc2\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# extract txt from original csv dataset\n",
        "# commented out as only needed the first time\n",
        "\n",
        "# import pandas as pd\n",
        "# abcnews = pd.read_csv('/content/drive/My Drive/abc/abcnews.csv')\n",
        "\n",
        "# with open(\"/content/drive/My Drive/abc/abcheads.txt\", \"a\") as file:\n",
        "#   for line in abcnews['headline_text']:\n",
        "#     line += \"\\n\"\n",
        "#     file.write(line)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import premade tokenizer (byte-level BPE)\n",
        "\n",
        "from transformers import RobertaTokenizer\n",
        "\n",
        "tokenizer_2b = RobertaTokenizer.from_pretrained('roberta-base')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# prepare txt dataset with tokenizer\n",
        "\n",
        "from transformers import LineByLineTextDataset\n",
        "\n",
        "dataset_2b = LineByLineTextDataset(\n",
        "    tokenizer=tokenizer_2b,\n",
        "    file_path=\"/content/drive/My Drive/abc/abcheads.txt\",\n",
        "    block_size=128,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# prepare data collator (speeds up things, not sure what)\n",
        "\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "data_collator_2b = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer_2b, mlm=True, mlm_probability=0.15\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create Roberta model from scratch, just prepare config\n",
        "\n",
        "from transformers import RobertaConfig, RobertaForMaskedLM\n",
        "\n",
        "config_2b = RobertaConfig(\n",
        "    vocab_size=52_000,\n",
        "    max_position_embeddings=514,\n",
        "    num_attention_heads=12,\n",
        "    num_hidden_layers=6,\n",
        "    type_vocab_size=1,\n",
        ")\n",
        "\n",
        "model_2b = RobertaForMaskedLM(config=config_2b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# set up training configurations\n",
        "\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args_2b = TrainingArguments(\n",
        "    output_dir=\"/content/drive/My Drive/abc/aBERTc2\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=64,\n",
        "    save_steps=10_000,\n",
        "    save_total_limit=2,\n",
        "    seed=1\n",
        ")\n",
        "\n",
        "trainer_2b = Trainer(\n",
        "    model=model_2b,\n",
        "    args=training_args_2b,\n",
        "    data_collator=data_collator_2b,\n",
        "    train_dataset=dataset_2b\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# verify model parameters ~ 84M\n",
        "\n",
        "model_2b.num_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# train and save trained model\n",
        "\n",
        "trainer_2b.train()\n",
        "trainer_2b.save_model(\"/content/drive/My Drive/abc/aBERTc2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# sanity check on model performance\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "fill_mask = pipeline(\n",
        "    \"fill-mask\",\n",
        "    model=\"/content/drive/My Drive/abc/aBERTc2\",\n",
        "    tokenizer=\"roberta-base\"\n",
        ")\n",
        "fill_mask(\"Send these <mask> back!\")"
      ]
    },
    {
      "source": [
        "At this point we tried replacing the newly trained RoBERTa model in the pipeline we had designed for Approach 1.\n",
        "\n",
        "(Follows Samy's code and explanation of what didn't work)"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "## c. FFN with word embeddings\n",
        "As second attempt we tried a modified version of a 2-layered feed-forward neural networks, modified from the 2nd lab of the course. We substituted the classification layer at the top with another linear layer in order to make it appropriate for the regression. We used all the words from the original headline + the edited word, stripped of its punctuation."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# recreate tok_corpus, word2idx and related functions, with punctuation removal\n",
        "\n",
        "re_punctuation_string = '[\\s,/.\\'<>]'\n",
        "\n",
        "def get_tokenized_corpus(corpus):\n",
        "  tokenized_corpus = []\n",
        "\n",
        "  for sentence in corpus:\n",
        "    tokenized_sentence = []\n",
        "    for token in re.split(' ', sentence): \n",
        "      tokenized_sentence.append(token)\n",
        "    tokenized_corpus.append(tokenized_sentence)\n",
        " \n",
        "  return tokenized_corpus\n",
        "\n",
        "\n",
        "def get_word2idx(tokenized_corpus):\n",
        "  vocabulary = []\n",
        "  for sentence in tokenized_corpus:\n",
        "    for token in sentence:\n",
        "        if token not in vocabulary:\n",
        "            vocabulary.append(token)\n",
        "  \n",
        "  word2idx = {w: idx+1 for (idx, w) in enumerate(vocabulary)}\n",
        "  word2idx['<pad>'] = 0\n",
        "  \n",
        "  return word2idx\n",
        "\n",
        "\n",
        "def get_model_inputs(tokenized_corpus, word2idx, labels):\n",
        "  vectorized_sents = [[word2idx[tok] for tok in sent if tok in word2idx] for sent in tokenized_corpus]\n",
        "  sent_lengths = [len(sent) for sent in vectorized_sents]\n",
        "  max_len = max(sent_lengths)\n",
        "  sent_tensor = torch.zeros((len(vectorized_sents), max_len)).long()\n",
        "  for idx, (sent, sentlen) in enumerate(zip(vectorized_sents, sent_lengths)):\n",
        "    sent_tensor[idx, :sentlen] = torch.LongTensor(sent)\n",
        "  label_tensor = torch.FloatTensor(labels)\n",
        "  return sent_tensor, label_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# construct network model\n",
        "\n",
        "class FFNN_2c(nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_dim, vocab_size):  \n",
        "        super(FFNN, self).__init__()\n",
        "\n",
        "        # padding_idx argument makes sure that the 0-th token in the vocabulary\n",
        "        # is used for padding purposes i.e. its embedding will be a 0-vector\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.fc1 = nn.Linear(embedding_dim, hidden_dim)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_dim, 1)\n",
        "        self.relu2 = nn.ReLU()  \n",
        "    \n",
        "    def forward(self, x):\n",
        "        # x has shape (batch_size, max_sent_len)\n",
        "\n",
        "        embedded = self.embedding(x)\n",
        "        sent_lens = x.ne(0).sum(1, keepdims=True)\n",
        "        averaged = embedded.sum(1) / sent_lens\n",
        "\n",
        "        out = self.fc1(averaged)\n",
        "        out = self.relu1(out)\n",
        "        out = self.fc2(out)\n",
        "        out = self.relu2(out)\n",
        "        out = torch.clamp(out, max=3.0)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# prepare traning set\n",
        "\n",
        "tokenized_corpus = get_tokenized_corpus(train_df[\"original\"].tolist().append(train_df[\"edit\"].tolist()))\n",
        "word2idx = get_word2idx(tokenized_corpus)\n",
        "train_sent_tensor, train_label_tensor = get_model_inputs(tokenized_corpus, word2idx, train_df['meanGrade'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# run model on training dataset\n",
        "\n",
        "EPOCHS = 10\n",
        "LRATE = 0.5\n",
        "EMBEDDING_DIM = 80\n",
        "HIDDEN_DIM = 80\n",
        "\n",
        "model_2c = FFNN_2c(EMBEDDING_DIM, HIDDEN_DIM, len(word2idx))\n",
        "optimizer_2c = torch.optim.SGD(model_2c.parameters(), lr=LRATE)\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "feature = train_sent_tensor\n",
        "target = train_label_tensor\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "  model_2c.train()\n",
        "  optimizer_2c.zero_grad()\n",
        "  \n",
        "  predictions = model_2c(feature).squeeze(1)\n",
        "  loss = loss_fn(predictions, target)\n",
        "  train_loss = loss.item()\n",
        "  \n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  \n",
        "  print(f'| Epoch: {epoch:02} | Train Loss: {train_loss:.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# prepare validation dataset\n",
        "\n",
        "dev_tokenized_corpus = get_tokenized_corpus(dev_df[\"original\"].tolist().append(dev_df[\"edit\"].tolist()))\n",
        "dev_sent_tensor, dev_label_tensor = get_model_inputs(dev_tokenized_corpus, word2idx, dev_df['meanGrade'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# optimise hyperparameters on validation set\n",
        "\n",
        "EPOCHS = 30\n",
        "LRATE = 0.5\n",
        "EMBEDDING_DIM = 50\n",
        "HIDDEN_DIM = 50\n",
        "OUTPUT_DIM = 1\n",
        "\n",
        "model_2c = FFNN_2c(EMBEDDING_DIM, HIDDEN_DIM, len(word2idx))\n",
        "optimizer_2c = torch.optim.SGD(model_2c.parameters(), lr=LRATE)\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "feature_train = train_sent_tensor\n",
        "target_train = train_label_tensor\n",
        "\n",
        "feature_valid = dev_sent_tensor\n",
        "target_valid = dev_label_tensor\n",
        "\n",
        "print(f'Will train for {EPOCHS} epochs')\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "  model_2c.train()\n",
        "  optimizer_2c.zero_grad()\n",
        "  predictions = model_2c(feature_train).squeeze(1)\n",
        "\n",
        "  loss = loss_fn(predictions, target_train)\n",
        "  train_loss = loss.item()\n",
        "\n",
        "  loss.backward()\n",
        "  optimizer_2c.step()\n",
        "  \n",
        "  model_2c.eval()\n",
        "  with torch.no_grad():\n",
        "    predictions_valid = model_2c(feature_valid).squeeze(1)\n",
        "    valid_loss = loss_fn(predictions_valid, target_valid).item()\n",
        "  \n",
        "  print(f'| Epoch: {epoch:02} | Train Loss: {train_loss:.3f} | Val. Loss: {valid_loss:.3f} |')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## d. Freestyle approach with handcrafter engineered features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "punctuations = '''!()-[]{};:ââ\"\\,.?@#$%^&*_~'''\n",
        "sex_words = [\"orgy\", \"dick\", \"suck\", \"underwear\", \"sex\", \"sexual\", \"massage\", \"flirt\", \"kiss\", \"ass\", \"penis\"]\n",
        "\n",
        "def create_engineered_features(train_df, punctuations, sex_words):\n",
        "    train_df[\"lower_case\"] = train_df.apply(\n",
        "            lambda x: x[\"original\"].lower(), axis=1\n",
        "        )\n",
        "    train_df[\"edit_lower\"] = train_df.apply(\n",
        "            lambda x: x[\"edit\"].lower(), axis=1\n",
        "        )\n",
        "    train_df[\"nb_words\"] = train_df.apply(\n",
        "            lambda x: len(x[\"lower_case\"].split(\" \")), axis=1\n",
        "        )\n",
        "    train_df[\"nb_caracters\"] = train_df.apply(\n",
        "            lambda x: len(x[\"lower_case\"]), axis=1\n",
        "        )\n",
        "    train_df[\"nb_ponctuations\"] = train_df.apply(\n",
        "            lambda x: sum([1 if char in punctuations else 0 for char in x[\"lower_case\"]]), axis=1\n",
        "        )\n",
        "    train_df[\"edit_position\"] = train_df.apply(\n",
        "            lambda x: x[\"lower_case\"].find(\"<\"), axis=1\n",
        "        )\n",
        "    train_df[\"edit_rel_position\"] = train_df.apply(\n",
        "            lambda x: x[\"edit_position\"]/x[\"nb_caracters\"], axis=1\n",
        "        )\n",
        "    train_df[\"len_edit\"] = train_df.apply(\n",
        "            lambda x: len(x[\"edit_lower\"]), axis=1\n",
        "        )\n",
        "    # i need trump\n",
        "    train_df[\"trump_in_original\"] = train_df.apply(\n",
        "            lambda x: float(x[\"lower_case\"].find(\"trump\") != -1), axis=1\n",
        "        )\n",
        "    train_df[\"trump_in_edit\"] = train_df.apply(\n",
        "            lambda x: float(x[\"edit_lower\"].find(\"trump\") != -1), axis=1\n",
        "        )\n",
        "\n",
        "    # i need hair\n",
        "    train_df[\"hair_in_original\"] = train_df.apply(\n",
        "            lambda x: float(x[\"lower_case\"].find(\"hair\") != -1), axis=1\n",
        "        )\n",
        "    train_df[\"hair_in_edit\"] = train_df.apply(\n",
        "            lambda x: float(x[\"edit_lower\"].find(\"hair\") != -1), axis=1\n",
        "        )\n",
        "    train_df[\"sex_in_edit\"] = train_df.apply(\n",
        "            lambda x: float(x[\"edit_lower\"] in sex_words), axis=1\n",
        "        )\n",
        "\n",
        "    return train_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_df = create_engineered_features(train_df, punctuations, sex_words)\n",
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# extract labels and features from training set \n",
        "\n",
        "columns = [\n",
        "    \"nb_words\",\n",
        "    \"nb_caracters\",\n",
        "    \"nb_ponctuations\",\n",
        "    \"edit_position\",\n",
        "    \"edit_rel_position\",\n",
        "    \"len_edit\",\t\"trump_in_original\",\n",
        "    \"trump_in_edit\",\n",
        "    \"hair_in_original\",\n",
        "    \"hair_in_edit\",\n",
        "    \"sex_in_edit\"\n",
        "    ]\n",
        "    \n",
        "features = train_df[columns].values\n",
        "labels = train_df.meanGrade.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# prepare the validation set\n",
        "\n",
        "dev_df = pd.read_csv('dev.csv')\n",
        "dev_df = create_engineered_features(dev_df, punctuations, sex_words)\n",
        "dev_inputs = dev_df[columns].values\n",
        "dev_labels = dev_df['meanGrade'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create dataloaders\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_inputs = torch.tensor(features, dtype=torch.float32)\n",
        "train_labels = torch.tensor(labels, dtype=torch.float32)\n",
        "\n",
        "validation_inputs = torch.tensor(dev_inputs, dtype=torch.float32)\n",
        "validation_labels = torch.tensor(dev_labels, dtype=torch.float32)\n",
        "\n",
        "train_data = torch.utils.data.TensorDataset(train_inputs, train_labels)\n",
        "dev_data = torch.utils.data.TensorDataset(validation_inputs, validation_labels)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_data, shuffle=True, batch_size=BATCH_SIZE)\n",
        "validation_loader = torch.utils.data.DataLoader(dev_data, shuffle=False, batch_size=BATCH_SIZE)\n",
        "\n",
        "print(\"Dataloaders created.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# design model architecture in torch\n",
        "\n",
        "class FFNN_2d(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(FunninessRegressor, self).__init__()\n",
        "\n",
        "        self.fc1 = torch.nn.Linear(11, 10)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.fc2 = torch.nn.Linear(10, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        out = self.fc1(x)\n",
        "        out = self.relu1(out)\n",
        "        out = self.fc2(out)\n",
        "        out = torch.clamp(out, min=0.0, max=3.0)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# set hyperparameters, model, optimizer and loss function\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "num_epochs = 2 #2\n",
        "learning_rate = 1.8e-6 #2e-6\n",
        "adam_eps = 1e-8\n",
        "\n",
        "model_2d = FFNN_2d()\n",
        "model_2d.to(device)\n",
        "model_2d.train()\n",
        "optimizer_2d = optim.Adam(model_2d.parameters(), lr=learning_rate, eps=adam_eps)\n",
        "\n",
        "loss_fn = nn.MSELoss()\n",
        "loss_fn = loss_fn.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# train model\n",
        "\n",
        "train(model, train_loader, validation_loader, num_epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# make predictions\n",
        "train_predictions = []\n",
        "model_perf = 0\n",
        "mean_perf = 0\n",
        "true_labels = []\n",
        "with torch.no_grad():\n",
        "    for batch in train_loader:\n",
        "        # add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        # unzip\n",
        "        input_ids, targets = batch\n",
        "        # predict (forward pass)\n",
        "        pred = model_2d(input_ids)\n",
        "        train_predictions += [pred]\n",
        "        true_labels += [targets]\n",
        "\n",
        "        mean_predictions = torch.ones_like(pred) * mean_value\n",
        "        mean_predictions = mean_predictions.to(device)\n",
        "\n",
        "        # for the model\n",
        "        # get scores\n",
        "        pred = pred.squeeze(1)\n",
        "        train_loss = loss_fn(pred, targets)\n",
        "\n",
        "        # We get the mse\n",
        "        pred, trg = pred.detach().cpu().numpy(), targets.detach().cpu().numpy()\n",
        "        sse, __ = model_performance(pred, trg)\n",
        "        model_perf += sse\n",
        "\n",
        "        # for the mean prediction\n",
        "        \n",
        "        # get scores\n",
        "        mean_predictions = mean_predictions.squeeze(1)\n",
        "        mean_loss = loss_fn(mean_predictions, targets)\n",
        "\n",
        "        # We get the mse\n",
        "        pred_mean, trg_mean = mean_predictions.detach().cpu().numpy(), targets.detach().cpu().numpy()\n",
        "        sse_mean, __ = model_performance(pred_mean, trg_mean)\n",
        "        mean_perf += sse_mean\n",
        "\n",
        "\n",
        "train_predictions = torch.cat(train_predictions, dim=0)\n",
        "true_labels = torch.cat(true_labels, dim=0).unsqueeze(1)\n",
        "check_values = torch.cat((train_predictions, true_labels), dim=1)\n",
        "print(\"Performances : --- Model : {} --- --- Mean predictor : {} ---\".format(model_perf, mean_perf))\n",
        "print(\"Values : \", check_values)X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# make predictions\n",
        "validation_predictions = []\n",
        "model_perf = 0\n",
        "mean_perf = 0\n",
        "true_labels = []\n",
        "with torch.no_grad():\n",
        "    for batch in validation_loader:\n",
        "        # add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        # unzip\n",
        "        input_ids, targets = batch\n",
        "        # predict (forward pass)\n",
        "        pred = model_2d(input_ids)\n",
        "        validation_predictions += [pred]\n",
        "        true_labels += [targets]\n",
        "\n",
        "        mean_predictions = torch.ones_like(pred) * mean_value\n",
        "        mean_predictions = mean_predictions.to(device)\n",
        "\n",
        "        # for the model\n",
        "        # get scores\n",
        "        pred = pred.squeeze(1)\n",
        "        validation_loss = loss_fn(pred, targets)\n",
        "\n",
        "        # We get the mse\n",
        "        pred, trg = pred.detach().cpu().numpy(), targets.detach().cpu().numpy()\n",
        "        sse, __ = model_performance(pred, trg)\n",
        "        model_perf += sse\n",
        "\n",
        "        # for the mean prediction\n",
        "        \n",
        "        # get scores\n",
        "        mean_predictions = mean_predictions.squeeze(1)\n",
        "        mean_loss = loss_fn(mean_predictions, targets)\n",
        "\n",
        "        # We get the mse\n",
        "        pred_mean, trg_mean = mean_predictions.detach().cpu().numpy(), targets.detach().cpu().numpy()\n",
        "        sse_mean, __ = model_performance(pred_mean, trg_mean)\n",
        "        mean_perf += sse_mean\n",
        "\n",
        "\n",
        "validation_predictions = torch.cat(validation_predictions, dim=0)\n",
        "true_labels = torch.cat(true_labels, dim=0).unsqueeze(1)\n",
        "check_values = torch.cat((validation_predictions, true_labels), dim=1)\n",
        "print(\"Performances : --- Model : {} --- --- Mean predictor : {} ---\".format(model_perf, mean_perf))\n",
        "print(\"Values : \", check_values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# get the data\n",
        "test_df = pd.read_csv('test.csv')\n",
        "# transforms the headlines\n",
        "test_df = create_engineered_features(test_df, punctuations, sex_words)\n",
        "test_input_ids = test_df[columns].values\n",
        "\n",
        "# convert to tensor\n",
        "test_inputs = torch.tensor(test_input_ids)\n",
        "\n",
        "# create \n",
        "test_data = torch.utils.data.TensorDataset(test_inputs)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, shuffle=False, batch_size=BATCH_SIZE)\n",
        "\n",
        "# make predictions\n",
        "test_predictions = []\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        # add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        # unzip\n",
        "        input_ids = batch\n",
        "        # predict (forward pass)\n",
        "        pred = model_2d(input_ids)\n",
        "        test_predictions += [pred]\n",
        "\n",
        "test_predictions = torch.cat(test_predictions, dim=0)\n",
        "test_predictions"
      ]
    }
  ]
}