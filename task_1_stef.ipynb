{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.2-final"},"colab":{"name":"task_1_stef.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true}},"cells":[{"cell_type":"markdown","metadata":{"id":"2TvyemfDlDmu"},"source":["### Coursework coding instructions (please also see full coursework spec)\n","\n","Please choose if you want to do either Task 1 or Task 2. You should write your report about one task only.\n","\n","For the task you choose you will need to do two approaches:\n","  - Approach 1, which can use use pre-trained embeddings / models\n","  - Approach 2, which should not use any pre-trained embeddings or models\n","We should be able to run both approaches from the same colab file\n","\n","#### Running your code:\n","  - Your models should run automatically when running your colab file without further intervention\n","  - For each task you should automatically output the performance of both models\n","  - Your code should automatically download any libraries required\n","\n","#### Structure of your code:\n","  - You are expected to use the 'train', 'eval' and 'model_performance' functions, although you may edit these as required\n","  - Otherwise there are no restrictions on what you can do in your code\n","\n","#### Documentation:\n","  - You are expected to produce a .README file summarising how you have approached both tasks\n","\n","#### Reproducibility:\n","  - Your .README file should explain how to replicate the different experiments mentioned in your report\n","\n","Good luck! We are really looking forward to seeing your reports and your model code!"]},{"cell_type":"code","metadata":{"id":"WX9TqmK7lDoK","executionInfo":{"status":"ok","timestamp":1614745691031,"user_tz":-60,"elapsed":499,"user":{"displayName":"Stefano Falini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9SFaTwMx5sTj1cR6VDhOI9M9PqTXOU2rNbwPcww=s64","userId":"05962169933495355141"}}},"source":["# Imports\n","\n","import torch\n","import torch.nn as nn\n","import pandas as pd\n","import numpy as np\n","from sklearn.feature_extraction.text import CountVectorizer\n","from torch.utils.data import Dataset, random_split\n","from sklearn.feature_extraction.text import TfidfTransformer\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LinearRegression\n","import codecs"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"X09jt8VRlDoM","executionInfo":{"status":"ok","timestamp":1614745691554,"user_tz":-60,"elapsed":1016,"user":{"displayName":"Stefano Falini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9SFaTwMx5sTj1cR6VDhOI9M9PqTXOU2rNbwPcww=s64","userId":"05962169933495355141"}}},"source":["# Setting random seed and device\n","SEED = 1\n","\n","torch.manual_seed(SEED)\n","torch.cuda.manual_seed(SEED)\n","torch.backends.cudnn.deterministic = True\n","\n","use_cuda = torch.cuda.is_available()\n","device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"AqhlzLl6lDoO","executionInfo":{"status":"ok","timestamp":1614745691554,"user_tz":-60,"elapsed":1012,"user":{"displayName":"Stefano Falini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9SFaTwMx5sTj1cR6VDhOI9M9PqTXOU2rNbwPcww=s64","userId":"05962169933495355141"}}},"source":["# Load data\n","train_df = pd.read_csv('train.csv')\n","dev_df = pd.read_csv('dev.csv')"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"3RCmF7xulDoP","executionInfo":{"status":"ok","timestamp":1614745691555,"user_tz":-60,"elapsed":1005,"user":{"displayName":"Stefano Falini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9SFaTwMx5sTj1cR6VDhOI9M9PqTXOU2rNbwPcww=s64","userId":"05962169933495355141"}}},"source":["# Number of epochs\n","epochs = 10\n","\n","# Proportion of training data for train compared to dev\n","train_proportion = 0.8"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"qAgZW6K1lDoR","executionInfo":{"status":"ok","timestamp":1614745691555,"user_tz":-60,"elapsed":1003,"user":{"displayName":"Stefano Falini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9SFaTwMx5sTj1cR6VDhOI9M9PqTXOU2rNbwPcww=s64","userId":"05962169933495355141"}}},"source":["# We define our training loop\n","def train(train_iter, dev_iter, model, number_epoch):\n","    \"\"\"\n","    Training loop for the model, which calls on eval to evaluate after each epoch\n","    \"\"\"\n","\n","    \n","    print(\"Training model.\")\n","\n","    for epoch in range(1, number_epoch+1):\n","\n","        model.train()\n","        epoch_loss = 0\n","        epoch_sse = 0\n","        no_observations = 0  # Observations used for training so far\n","\n","        for batch in train_iter:\n","\n","            feature, target = batch\n","\n","            feature, target = feature.to(device), target.to(device)\n","\n","            # for RNN:\n","            model.batch_size = target.shape[0]\n","            no_observations = no_observations + target.shape[0]\n","            model.hidden = model.init_hidden()\n","\n","            predictions = model(feature).squeeze(1)\n","\n","            optimizer.zero_grad()\n","\n","            loss = loss_fn(predictions, target)\n","\n","            sse, __ = model_performance(predictions.detach().cpu().numpy(), target.detach().cpu().numpy())\n","\n","            loss.backward()\n","            optimizer.step()\n","\n","            epoch_loss += loss.item()*target.shape[0]\n","            epoch_sse += sse\n","\n","        valid_loss, valid_mse, __, __ = eval(dev_iter, model)\n","\n","        epoch_loss, epoch_mse = epoch_loss / no_observations, epoch_sse / no_observations\n","        print(f'| Epoch: {epoch:02} | Train Loss: {epoch_loss:.2f} | Train MSE: {epoch_mse:.2f} | Train RMSE: {epoch_mse**0.5:.2f} | \\\n","        Val. Loss: {valid_loss:.2f} | Val. MSE: {valid_mse:.2f} |  Val. RMSE: {valid_mse**0.5:.2f} |')"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"NzXeDgHmlDob","executionInfo":{"status":"ok","timestamp":1614745691556,"user_tz":-60,"elapsed":1002,"user":{"displayName":"Stefano Falini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9SFaTwMx5sTj1cR6VDhOI9M9PqTXOU2rNbwPcww=s64","userId":"05962169933495355141"}}},"source":["# We evaluate performance on our dev set\n","def eval(data_iter, model):\n","    \"\"\"\n","    Evaluating model performance on the dev set\n","    \"\"\"\n","    model.eval()\n","    epoch_loss = 0\n","    epoch_sse = 0\n","    pred_all = []\n","    trg_all = []\n","    no_observations = 0\n","\n","    with torch.no_grad():\n","        for batch in data_iter:\n","            feature, target = batch\n","\n","            feature, target = feature.to(device), target.to(device)\n","\n","            # for RNN:\n","            model.batch_size = target.shape[0]\n","            no_observations = no_observations + target.shape[0]\n","            model.hidden = model.init_hidden()\n","\n","            predictions = model(feature).squeeze(1)\n","            loss = loss_fn(predictions, target)\n","\n","            # We get the mse\n","            pred, trg = predictions.detach().cpu().numpy(), target.detach().cpu().numpy()\n","            sse, __ = model_performance(pred, trg)\n","\n","            epoch_loss += loss.item()*target.shape[0]\n","            epoch_sse += sse\n","            pred_all.extend(pred)\n","            trg_all.extend(trg)\n","\n","    return epoch_loss/no_observations, epoch_sse/no_observations, np.array(pred_all), np.array(trg_all)"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"2_22fHHElDog","executionInfo":{"status":"ok","timestamp":1614745691556,"user_tz":-60,"elapsed":1000,"user":{"displayName":"Stefano Falini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9SFaTwMx5sTj1cR6VDhOI9M9PqTXOU2rNbwPcww=s64","userId":"05962169933495355141"}}},"source":["# How we print the model performance\n","def model_performance(output, target, print_output=False):\n","    \"\"\"\n","    Returns SSE and MSE per batch (printing the MSE and the RMSE)\n","    \"\"\"\n","\n","    sq_error = (output - target)**2\n","\n","    sse = np.sum(sq_error)\n","    mse = np.mean(sq_error)\n","    rmse = np.sqrt(mse)\n","\n","    if print_output:\n","        print(f'| MSE: {mse:.2f} | RMSE: {rmse:.2f} |')\n","\n","    return sse, mse"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"LcxmqrKhlDoj","executionInfo":{"status":"ok","timestamp":1614745691556,"user_tz":-60,"elapsed":999,"user":{"displayName":"Stefano Falini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9SFaTwMx5sTj1cR6VDhOI9M9PqTXOU2rNbwPcww=s64","userId":"05962169933495355141"}}},"source":["def create_vocab(data):\n","    \"\"\"\n","    Creating a corpus of all the tokens used\n","    \"\"\"\n","    tokenized_corpus = [] # Let us put the tokenized corpus in a list\n","\n","    for sentence in data:\n","\n","        tokenized_sentence = []\n","\n","        for token in sentence.split(' '): # simplest split is\n","\n","            tokenized_sentence.append(token)\n","\n","        tokenized_corpus.append(tokenized_sentence)\n","\n","    # Create single list of all vocabulary\n","    vocabulary = []  # Let us put all the tokens (mostly words) appearing in the vocabulary in a list\n","\n","    for sentence in tokenized_corpus:\n","\n","        for token in sentence:\n","\n","            if token not in vocabulary:\n","\n","                if True:\n","                    vocabulary.append(token)\n","\n","    return vocabulary, tokenized_corpus"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"jzQ0KLXslDoq","executionInfo":{"status":"ok","timestamp":1614745691557,"user_tz":-60,"elapsed":998,"user":{"displayName":"Stefano Falini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9SFaTwMx5sTj1cR6VDhOI9M9PqTXOU2rNbwPcww=s64","userId":"05962169933495355141"}}},"source":["def collate_fn_padd(batch):\n","    '''\n","    We add padding to our minibatches and create tensors for our model\n","    '''\n","\n","    batch_labels = [l for f, l in batch]\n","    batch_features = [f for f, l in batch]\n","\n","    batch_features_len = [len(f) for f, l in batch]\n","\n","    seq_tensor = torch.zeros((len(batch), max(batch_features_len))).long()\n","\n","    for idx, (seq, seqlen) in enumerate(zip(batch_features, batch_features_len)):\n","        seq_tensor[idx, :seqlen] = torch.LongTensor(seq)\n","\n","    batch_labels = torch.FloatTensor(batch_labels)\n","\n","    return seq_tensor, batch_labels\n","\n","class Task1Dataset(Dataset):\n","\n","    def __init__(self, train_data, labels):\n","        self.x_train = train_data\n","        self.y_train = labels\n","\n","    def __len__(self):\n","        return len(self.y_train)\n","\n","    def __getitem__(self, item):\n","        return self.x_train[item], self.y_train[item]"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rdeFaoc3lDpK"},"source":["#### Approach 2: No pre-trained representations"]},{"cell_type":"code","metadata":{"id":"46gm47T4lDpQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614745691782,"user_tz":-60,"elapsed":1221,"user":{"displayName":"Stefano Falini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9SFaTwMx5sTj1cR6VDhOI9M9PqTXOU2rNbwPcww=s64","userId":"05962169933495355141"}},"outputId":"1ec41827-d2d4-44d1-e5e7-3a1da6fbc289"},"source":["train_and_dev = train_df['edit']\n","\n","training_data, dev_data, training_y, dev_y = train_test_split(train_df['edit'], train_df['meanGrade'],\n","                                                                        test_size=(1-train_proportion),\n","                                                                        random_state=42)\n","\n","# We train a Tf-idf model\n","count_vect = CountVectorizer(stop_words='english')\n","train_counts = count_vect.fit_transform(training_data)\n","transformer = TfidfTransformer().fit(train_counts)\n","train_counts = transformer.transform(train_counts)\n","regression_model = LinearRegression().fit(train_counts, training_y)\n","\n","# Train predictions\n","predicted_train = regression_model.predict(train_counts)\n","\n","# Calculate Tf-idf using train and dev, and validate model on dev:\n","test_and_test_counts = count_vect.transform(train_and_dev)\n","transformer = TfidfTransformer().fit(test_and_test_counts)\n","\n","test_counts = count_vect.transform(dev_data)\n","\n","test_counts = transformer.transform(test_counts)\n","\n","# Dev predictions\n","predicted = regression_model.predict(test_counts)\n","\n","# We run the evaluation:\n","print(\"\\nTrain performance:\")\n","sse, mse = model_performance(predicted_train, training_y, True)\n","\n","print(\"\\nDev performance:\")\n","sse, mse = model_performance(predicted, dev_y, True)"],"execution_count":13,"outputs":[{"output_type":"stream","text":["\n","Train performance:\n","| MSE: 0.13 | RMSE: 0.37 |\n","\n","Dev performance:\n","| MSE: 0.36 | RMSE: 0.60 |\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"9HyHwHkUlDpa"},"source":["#### Baseline for task 2"]},{"cell_type":"code","metadata":{"id":"7DA3q4o1lDpd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614745691783,"user_tz":-60,"elapsed":1221,"user":{"displayName":"Stefano Falini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9SFaTwMx5sTj1cR6VDhOI9M9PqTXOU2rNbwPcww=s64","userId":"05962169933495355141"}},"outputId":"facbc7c7-308a-4056-9e4a-051d0249a715"},"source":["#Â Baseline for the task\n","pred_baseline = torch.zeros(len(dev_y)) + np.mean(training_y)\n","print(\"\\nBaseline performance:\")\n","sse, mse = model_performance(pred_baseline, dev_y, True)"],"execution_count":14,"outputs":[{"output_type":"stream","text":["\n","Baseline performance:\n","| MSE: 0.34 | RMSE: 0.58 |\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"C2GMPxu75KAT","executionInfo":{"status":"ok","timestamp":1614745691993,"user_tz":-60,"elapsed":1429,"user":{"displayName":"Stefano Falini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9SFaTwMx5sTj1cR6VDhOI9M9PqTXOU2rNbwPcww=s64","userId":"05962169933495355141"}}},"source":["import re\n","\n","re_punctuation_string = '[\\s,/.\\'<>]'\n","\n","def get_tokenized_corpus(corpus):\n","  tokenized_corpus = []\n","\n","  for sentence in corpus:\n","    tokenized_sentence = []\n","    for token in re.split(' ', sentence): \n","      tokenized_sentence.append(token)\n","    tokenized_corpus.append(tokenized_sentence)\n"," \n","  return tokenized_corpus\n","\n","\n","def get_word2idx(tokenized_corpus):\n","  vocabulary = []\n","  for sentence in tokenized_corpus:\n","    for token in sentence:\n","        if token not in vocabulary:\n","            vocabulary.append(token)\n","  \n","  word2idx = {w: idx+1 for (idx, w) in enumerate(vocabulary)}\n","  word2idx['<pad>'] = 0\n","  \n","  return word2idx\n","\n","\n","def get_model_inputs(tokenized_corpus, word2idx, labels):\n","  vectorized_sents = [[word2idx[tok] for tok in sent if tok in word2idx] for sent in tokenized_corpus]\n","  sent_lengths = [len(sent) for sent in vectorized_sents]\n","  max_len = max(sent_lengths)\n","  sent_tensor = torch.zeros((len(vectorized_sents), max_len)).long()\n","  for idx, (sent, sentlen) in enumerate(zip(vectorized_sents, sent_lengths)):\n","    sent_tensor[idx, :sentlen] = torch.LongTensor(sent)\n","  label_tensor = torch.FloatTensor(labels)\n","  return sent_tensor, label_tensor"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"X_9VyYXP5KAU","executionInfo":{"status":"ok","timestamp":1614745697695,"user_tz":-60,"elapsed":7130,"user":{"displayName":"Stefano Falini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9SFaTwMx5sTj1cR6VDhOI9M9PqTXOU2rNbwPcww=s64","userId":"05962169933495355141"}}},"source":["tokenized_corpus = get_tokenized_corpus(train_df[\"original\"].tolist())\n","word2idx = get_word2idx(tokenized_corpus)\n","train_sent_tensor, train_label_tensor = get_model_inputs(tokenized_corpus, word2idx, train_df['meanGrade'])"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"84nQDZyBlDpg","executionInfo":{"status":"ok","timestamp":1614745697695,"user_tz":-60,"elapsed":7128,"user":{"displayName":"Stefano Falini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9SFaTwMx5sTj1cR6VDhOI9M9PqTXOU2rNbwPcww=s64","userId":"05962169933495355141"}}},"source":["class FFNN(nn.Module):\n","    def __init__(self, embedding_dim, hidden_dim, vocab_size):  \n","        super(FFNN, self).__init__()\n","\n","        # padding_idx argument makes sure that the 0-th token in the vocabulary\n","        # is used for padding purposes i.e. its embedding will be a 0-vector\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n","        self.fc1 = nn.Linear(embedding_dim, hidden_dim)\n","        self.relu1 = nn.ReLU()\n","        self.fc2 = nn.Linear(hidden_dim, 1)\n","        self.relu2 = nn.ReLU()  \n","    \n","    def forward(self, x):\n","        # x has shape (batch_size, max_sent_len)\n","\n","        embedded = self.embedding(x)\n","        sent_lens = x.ne(0).sum(1, keepdims=True)\n","        averaged = embedded.sum(1) / sent_lens\n","\n","        out = self.fc1(averaged)\n","        out = self.relu1(out)\n","        out = self.fc2(out)\n","        out = self.relu2(out)\n","        out = torch.clamp(out, max=3.0)\n","        return out"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"gacCx3Ex5KAU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614745701269,"user_tz":-60,"elapsed":10701,"user":{"displayName":"Stefano Falini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9SFaTwMx5sTj1cR6VDhOI9M9PqTXOU2rNbwPcww=s64","userId":"05962169933495355141"}},"outputId":"dec45ef5-bf72-4c98-a821-b9fcbc5a8785"},"source":["EPOCHS = 10\n","LRATE = 0.5\n","EMBEDDING_DIM = 80\n","HIDDEN_DIM = 80\n","\n","model = FFNN(EMBEDDING_DIM, HIDDEN_DIM, len(word2idx))\n","optimizer = torch.optim.SGD(model.parameters(), lr=LRATE)\n","loss_fn = nn.MSELoss()\n","\n","feature = train_sent_tensor\n","target = train_label_tensor\n","\n","for epoch in range(1, EPOCHS + 1):\n","  model.train()\n","  optimizer.zero_grad()\n","  \n","  predictions = model(feature).squeeze(1)\n","  loss = loss_fn(predictions, target)\n","  train_loss = loss.item()\n","  \n","  loss.backward()\n","  optimizer.step()\n","  \n","  print(f'| Epoch: {epoch:02} | Train Loss: {train_loss:.3f}')"],"execution_count":18,"outputs":[{"output_type":"stream","text":["| Epoch: 01 | Train Loss: 1.202\n","| Epoch: 02 | Train Loss: 0.766\n","| Epoch: 03 | Train Loss: 0.638\n","| Epoch: 04 | Train Loss: 0.628\n","| Epoch: 05 | Train Loss: 0.418\n","| Epoch: 06 | Train Loss: 0.373\n","| Epoch: 07 | Train Loss: 0.351\n","| Epoch: 08 | Train Loss: 0.345\n","| Epoch: 09 | Train Loss: 0.343\n","| Epoch: 10 | Train Loss: 0.342\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KC4EtcsACFa8","executionInfo":{"status":"ok","timestamp":1614745701269,"user_tz":-60,"elapsed":10700,"user":{"displayName":"Stefano Falini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9SFaTwMx5sTj1cR6VDhOI9M9PqTXOU2rNbwPcww=s64","userId":"05962169933495355141"}}},"source":["dev_tokenized_corpus = get_tokenized_corpus(dev_df[\"original\"].tolist())\n","dev_sent_tensor, dev_label_tensor = get_model_inputs(dev_tokenized_corpus, word2idx, dev_df['meanGrade'])"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8H8GSmbeDqyA","executionInfo":{"status":"ok","timestamp":1614745710564,"user_tz":-60,"elapsed":19993,"user":{"displayName":"Stefano Falini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9SFaTwMx5sTj1cR6VDhOI9M9PqTXOU2rNbwPcww=s64","userId":"05962169933495355141"}},"outputId":"7fe0dd30-370e-4564-b2e5-1fa80225964b"},"source":["EPOCHS = 30\n","LRATE = 0.5\n","EMBEDDING_DIM = 50\n","HIDDEN_DIM = 50\n","OUTPUT_DIM = 1\n","\n","model = FFNN(EMBEDDING_DIM, HIDDEN_DIM, len(word2idx))\n","optimizer = torch.optim.SGD(model.parameters(), lr=LRATE)\n","loss_fn = nn.MSELoss()\n","\n","feature_train = train_sent_tensor\n","target_train = train_label_tensor\n","\n","feature_valid = dev_sent_tensor\n","target_valid = dev_label_tensor\n","\n","print(f'Will train for {EPOCHS} epochs')\n","for epoch in range(1, EPOCHS + 1):\n","  model.train()\n","  optimizer.zero_grad()\n","  predictions = model(feature_train).squeeze(1)\n","\n","  loss = loss_fn(predictions, target_train)\n","  train_loss = loss.item()\n","\n","  loss.backward()\n","  optimizer.step()\n","  \n","  model.eval()\n","  with torch.no_grad():\n","    predictions_valid = model(feature_valid).squeeze(1)\n","    valid_loss = loss_fn(predictions_valid, target_valid).item()\n","  \n","  print(f'| Epoch: {epoch:02} | Train Loss: {train_loss:.3f} | Val. Loss: {valid_loss:.3f} |')"],"execution_count":20,"outputs":[{"output_type":"stream","text":["Will train for 30 epochs\n","| Epoch: 01 | Train Loss: 1.053 | Val. Loss: 0.565 |\n","| Epoch: 02 | Train Loss: 0.534 | Val. Loss: 0.473 |\n","| Epoch: 03 | Train Loss: 0.486 | Val. Loss: 0.387 |\n","| Epoch: 04 | Train Loss: 0.378 | Val. Loss: 0.351 |\n","| Epoch: 05 | Train Loss: 0.357 | Val. Loss: 0.349 |\n","| Epoch: 06 | Train Loss: 0.347 | Val. Loss: 0.342 |\n","| Epoch: 07 | Train Loss: 0.344 | Val. Loss: 0.343 |\n","| Epoch: 08 | Train Loss: 0.343 | Val. Loss: 0.341 |\n","| Epoch: 09 | Train Loss: 0.342 | Val. Loss: 0.341 |\n","| Epoch: 10 | Train Loss: 0.341 | Val. Loss: 0.341 |\n","| Epoch: 11 | Train Loss: 0.341 | Val. Loss: 0.341 |\n","| Epoch: 12 | Train Loss: 0.340 | Val. Loss: 0.340 |\n","| Epoch: 13 | Train Loss: 0.340 | Val. Loss: 0.340 |\n","| Epoch: 14 | Train Loss: 0.340 | Val. Loss: 0.340 |\n","| Epoch: 15 | Train Loss: 0.339 | Val. Loss: 0.340 |\n","| Epoch: 16 | Train Loss: 0.339 | Val. Loss: 0.340 |\n","| Epoch: 17 | Train Loss: 0.339 | Val. Loss: 0.340 |\n","| Epoch: 18 | Train Loss: 0.339 | Val. Loss: 0.340 |\n","| Epoch: 19 | Train Loss: 0.338 | Val. Loss: 0.340 |\n","| Epoch: 20 | Train Loss: 0.338 | Val. Loss: 0.340 |\n","| Epoch: 21 | Train Loss: 0.338 | Val. Loss: 0.339 |\n","| Epoch: 22 | Train Loss: 0.338 | Val. Loss: 0.339 |\n","| Epoch: 23 | Train Loss: 0.337 | Val. Loss: 0.339 |\n","| Epoch: 24 | Train Loss: 0.337 | Val. Loss: 0.339 |\n","| Epoch: 25 | Train Loss: 0.337 | Val. Loss: 0.339 |\n","| Epoch: 26 | Train Loss: 0.337 | Val. Loss: 0.339 |\n","| Epoch: 27 | Train Loss: 0.337 | Val. Loss: 0.339 |\n","| Epoch: 28 | Train Loss: 0.336 | Val. Loss: 0.339 |\n","| Epoch: 29 | Train Loss: 0.336 | Val. Loss: 0.339 |\n","| Epoch: 30 | Train Loss: 0.336 | Val. Loss: 0.339 |\n"],"name":"stdout"}]}]}