{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "attempted-subdivision",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from torch.utils.data import Dataset, random_split\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "published-peninsula",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, TFBertForSequenceClassification, BertModel\n",
    "import tensorflow\n",
    "\n",
    "SEED = 1\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "\n",
    "\n",
    "# Number of epochs\n",
    "epochs = 10\n",
    "\n",
    "# Proportion of training data for train compared to dev\n",
    "train_proportion = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "suited-shooting",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>original</th>\n",
       "      <th>edit</th>\n",
       "      <th>grades</th>\n",
       "      <th>meanGrade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14530</td>\n",
       "      <td>France is ‘ hunting down its citizens who join...</td>\n",
       "      <td>twins</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13034</td>\n",
       "      <td>Pentagon claims 2,000 % increase in Russian tr...</td>\n",
       "      <td>bowling</td>\n",
       "      <td>33110</td>\n",
       "      <td>1.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8731</td>\n",
       "      <td>Iceland PM Calls Snap Vote as Pedophile Furor ...</td>\n",
       "      <td>party</td>\n",
       "      <td>22100</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>76</td>\n",
       "      <td>In an apparent first , Iran and Israel &lt;engage...</td>\n",
       "      <td>slap</td>\n",
       "      <td>20000</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6164</td>\n",
       "      <td>Trump was told weeks ago that Flynn misled &lt;Vi...</td>\n",
       "      <td>school</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                           original     edit  grades  \\\n",
       "0  14530  France is ‘ hunting down its citizens who join...    twins   10000   \n",
       "1  13034  Pentagon claims 2,000 % increase in Russian tr...  bowling   33110   \n",
       "2   8731  Iceland PM Calls Snap Vote as Pedophile Furor ...    party   22100   \n",
       "3     76  In an apparent first , Iran and Israel <engage...     slap   20000   \n",
       "4   6164  Trump was told weeks ago that Flynn misled <Vi...   school       0   \n",
       "\n",
       "   meanGrade  \n",
       "0        0.2  \n",
       "1        1.6  \n",
       "2        1.0  \n",
       "3        0.4  \n",
       "4        0.0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import the data\n",
    "\n",
    "train_df = pd.read_csv('data/task-1/train.csv')\n",
    "test_df = pd.read_csv('data/task-1/dev.csv')\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "other-excerpt",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, batch_size, device):\n",
    "        super(Network, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2label = nn.Linear(hidden_dim * 2, 1)\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # Before we've done anything, we dont have any hidden state.\n",
    "        # Refer to the Pytorch documentation to see exactly why they have this dimensionality.\n",
    "        # The axes semantics are (num_layers * num_directions, minibatch_size, hidden_dim)\n",
    "        return torch.zeros(2, self.batch_size, self.hidden_dim).to(self.device), \\\n",
    "               torch.zeros(2, self.batch_size, self.hidden_dim).to(self.device)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # TODO\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "formal-protein",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define our training loop\n",
    "def train(train_iter, dev_iter, model, number_epoch):\n",
    "    \"\"\"\n",
    "    Training loop for the model, which calls on eval to evaluate after each epoch\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    print(\"Training model.\")\n",
    "\n",
    "    for epoch in range(1, number_epoch+1):\n",
    "\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        epoch_sse = 0\n",
    "        no_observations = 0  # Observations used for training so far\n",
    "\n",
    "        for batch in train_iter:\n",
    "\n",
    "            feature, target = batch\n",
    "\n",
    "            feature, target = feature.to(device), target.to(device)\n",
    "\n",
    "            # for RNN:\n",
    "            model.batch_size = target.shape[0]\n",
    "            no_observations = no_observations + target.shape[0]\n",
    "            model.hidden = model.init_hidden()\n",
    "\n",
    "            predictions = model(feature).squeeze(1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss = loss_fn(predictions, target)\n",
    "\n",
    "            sse, __ = model_performance(predictions.detach().cpu().numpy(), target.detach().cpu().numpy())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()*target.shape[0]\n",
    "            epoch_sse += sse\n",
    "\n",
    "        valid_loss, valid_mse, __, __ = eval(dev_iter, model)\n",
    "\n",
    "        epoch_loss, epoch_mse = epoch_loss / no_observations, epoch_sse / no_observations\n",
    "        print(f'| Epoch: {epoch:02} | Train Loss: {epoch_loss:.2f} | Train MSE: {epoch_mse:.2f} | Train RMSE: {epoch_mse**0.5:.2f} | \\\n",
    "        Val. Loss: {valid_loss:.2f} | Val. MSE: {valid_mse:.2f} |  Val. RMSE: {valid_mse**0.5:.2f} |')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "loose-spirituality",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We evaluate performance on our dev set\n",
    "def eval(data_iter, model):\n",
    "    \"\"\"\n",
    "    Evaluating model performance on the dev set\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    epoch_sse = 0\n",
    "    pred_all = []\n",
    "    trg_all = []\n",
    "    no_observations = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_iter:\n",
    "            feature, target = batch\n",
    "\n",
    "            feature, target = feature.to(device), target.to(device)\n",
    "\n",
    "            # for RNN:\n",
    "            model.batch_size = target.shape[0]\n",
    "            no_observations = no_observations + target.shape[0]\n",
    "            model.hidden = model.init_hidden()\n",
    "\n",
    "            predictions = model(feature).squeeze(1)\n",
    "            loss = loss_fn(predictions, target)\n",
    "\n",
    "            # We get the mse\n",
    "            pred, trg = predictions.detach().cpu().numpy(), target.detach().cpu().numpy()\n",
    "            sse, __ = model_performance(pred, trg)\n",
    "\n",
    "            epoch_loss += loss.item()*target.shape[0]\n",
    "            epoch_sse += sse\n",
    "            pred_all.extend(pred)\n",
    "            trg_all.extend(trg)\n",
    "\n",
    "    return epoch_loss/no_observations, epoch_sse/no_observations, np.array(pred_all), np.array(trg_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "threaded-handle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How we print the model performance\n",
    "def model_performance(output, target, print_output=False):\n",
    "    \"\"\"\n",
    "    Returns SSE and MSE per batch (printing the MSE and the RMSE)\n",
    "    \"\"\"\n",
    "\n",
    "    sq_error = (output - target)**2\n",
    "\n",
    "    sse = np.sum(sq_error)\n",
    "    mse = np.mean(sq_error)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    if print_output:\n",
    "        print(f'| MSE: {mse:.2f} | RMSE: {rmse:.2f} |')\n",
    "\n",
    "    return sse, mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "convenient-pottery",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn_padd(batch):\n",
    "    '''\n",
    "    We add padding to our minibatches and create tensors for our model\n",
    "    '''\n",
    "\n",
    "    batch_labels = [l for f, l in batch]\n",
    "    batch_features = [f for f, l in batch]\n",
    "\n",
    "    batch_features_len = [len(f) for f, l in batch]\n",
    "\n",
    "    seq_tensor = torch.zeros((len(batch), max(batch_features_len))).long()\n",
    "\n",
    "    for idx, (seq, seqlen) in enumerate(zip(batch_features, batch_features_len)):\n",
    "        seq_tensor[idx, :seqlen] = torch.LongTensor(seq)\n",
    "\n",
    "    batch_labels = torch.FloatTensor(batch_labels)\n",
    "\n",
    "    return seq_tensor, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "prime-geometry",
   "metadata": {},
   "outputs": [],
   "source": [
    "def headlines2vec(tokenizer, model, headlines):    \n",
    "    \"\"\"\n",
    "    Function that applies a tokenizer to the \n",
    "    headlines and padds the output to get the expected dimension\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"ret = np.array(tokenizer(headlines, padding = True, add_special_tokens = True)['input_ids'])\n",
    "    return ret\"\"\"\n",
    "    \n",
    "    input_ids = torch.tensor(tokenizer(headlines, padding = True)[\"input_ids\"]).unsqueeze(0)  # Batch size 1\n",
    "\n",
    "    outputs = model(input_ids[0])\n",
    "    last_hidden_states = outputs[0]\n",
    "\n",
    "    return last_hidden_states\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "roman-native",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialised.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "## Approach 1 code, using functions defined above:\n",
    "\n",
    "# We set our training data and test data\n",
    "training_data = train_df['original']\n",
    "test_data = test_df['original']\n",
    "\n",
    "\n",
    "\n",
    "INPUT_DIM = 20 #20 words max in a headline\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "model = Network(INPUT_DIM, 50, BATCH_SIZE, device)\n",
    "print(\"Model initialised.\")\n",
    "\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# EMBEDDING\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "train_dataset = headlines2vec(tokenizer, model, training_data.tolist()) #TOLIST AVOIDABLE ?\n",
    "\n",
    "print(train_dataset) #MEAN GRADE !!!\n",
    "\n",
    "dev_dataset = headlines2vec(tokenizer, model, test_data.tolist())\n",
    "\n",
    "print(\"Embedding done.\")\n",
    "\n",
    "\n",
    "# DATA LOADERS\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, shuffle=True, batch_size=BATCH_SIZE, collate_fn=collate_fn_padd)\n",
    "dev_loader = torch.utils.data.DataLoader(dev_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn_padd)\n",
    "\n",
    "print(\"Dataloaders created.\")\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "loss_fn = loss_fn.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "train(train_loader, dev_loader, model, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrong-decline",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
