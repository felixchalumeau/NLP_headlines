{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.2-final"},"colab":{"name":"task_1_main.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"2TvyemfDlDmu"},"source":["### Coursework coding instructions (please also see full coursework spec)\n","\n","Please choose if you want to do either Task 1 or Task 2. You should write your report about one task only.\n","\n","For the task you choose you will need to do two approaches:\n","  - Approach 1, which can use use pre-trained embeddings / models\n","  - Approach 2, which should not use any pre-trained embeddings or models\n","We should be able to run both approaches from the same colab file\n","\n","#### Running your code:\n","  - Your models should run automatically when running your colab file without further intervention\n","  - For each task you should automatically output the performance of both models\n","  - Your code should automatically download any libraries required\n","\n","#### Structure of your code:\n","  - You are expected to use the 'train', 'eval' and 'model_performance' functions, although you may edit these as required\n","  - Otherwise there are no restrictions on what you can do in your code\n","\n","#### Documentation:\n","  - You are expected to produce a .README file summarising how you have approached both tasks\n","\n","#### Reproducibility:\n","  - Your .README file should explain how to replicate the different experiments mentioned in your report\n","\n","Good luck! We are really looking forward to seeing your reports and your model code!"]},{"cell_type":"code","metadata":{"id":"LRWFk-kelDoA"},"source":["# You will need to download any word embeddings required for your code, e.g.:\n","\n","!wget http://nlp.stanford.edu/data/glove.6B.zip\n","!unzip glove.6B.zip\n","\n","# For any packages that Colab does not provide auotmatically you will also need to install these below, e.g.:\n","\n","#! pip install torch"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["/bin/bash: wget: command not found\n","unzip:  cannot find or open glove.6B.zip, glove.6B.zip.zip or glove.6B.zip.ZIP.\n"]}]},{"cell_type":"code","metadata":{"id":"WX9TqmK7lDoK"},"source":["# Imports\n","\n","import torch\n","import torch.nn as nn\n","import pandas as pd\n","import numpy as np\n","from sklearn.feature_extraction.text import CountVectorizer\n","from torch.utils.data import Dataset, random_split\n","from sklearn.feature_extraction.text import TfidfTransformer\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LinearRegression\n","import codecs"],"execution_count":7,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"module functions cannot set METH_CLASS or METH_STATIC","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-f4d6a8a0ab95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Imports\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mUSE_GLOBAL_DEPS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0m_load_global_deps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;31m# Appease the type checker; ordinarily this binding is inserted by the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: module functions cannot set METH_CLASS or METH_STATIC"]}]},{"cell_type":"code","metadata":{"id":"X09jt8VRlDoM"},"source":["# Setting random seed and device\n","SEED = 1\n","\n","torch.manual_seed(SEED)\n","torch.cuda.manual_seed(SEED)\n","torch.backends.cudnn.deterministic = True\n","\n","use_cuda = torch.cuda.is_available()\n","device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AqhlzLl6lDoO"},"source":["# Load data\n","train_df = pd.read_csv('data/task-1/train.csv')\n","test_df = pd.read_csv('data/task-1/dev.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3RCmF7xulDoP"},"source":["# Number of epochs\n","epochs = 10\n","\n","# Proportion of training data for train compared to dev\n","train_proportion = 0.8"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qAgZW6K1lDoR"},"source":["# We define our training loop\n","def train(train_iter, dev_iter, model, number_epoch):\n","    \"\"\"\n","    Training loop for the model, which calls on eval to evaluate after each epoch\n","    \"\"\"\n","\n","    \n","    print(\"Training model.\")\n","\n","    for epoch in range(1, number_epoch+1):\n","\n","        model.train()\n","        epoch_loss = 0\n","        epoch_sse = 0\n","        no_observations = 0  # Observations used for training so far\n","\n","        for batch in train_iter:\n","\n","            feature, target = batch\n","\n","            feature, target = feature.to(device), target.to(device)\n","\n","            # for RNN:\n","            model.batch_size = target.shape[0]\n","            no_observations = no_observations + target.shape[0]\n","            model.hidden = model.init_hidden()\n","\n","            predictions = model(feature).squeeze(1)\n","\n","            optimizer.zero_grad()\n","\n","            loss = loss_fn(predictions, target)\n","\n","            sse, __ = model_performance(predictions.detach().cpu().numpy(), target.detach().cpu().numpy())\n","\n","            loss.backward()\n","            optimizer.step()\n","\n","            epoch_loss += loss.item()*target.shape[0]\n","            epoch_sse += sse\n","\n","        valid_loss, valid_mse, __, __ = eval(dev_iter, model)\n","\n","        epoch_loss, epoch_mse = epoch_loss / no_observations, epoch_sse / no_observations\n","        print(f'| Epoch: {epoch:02} | Train Loss: {epoch_loss:.2f} | Train MSE: {epoch_mse:.2f} | Train RMSE: {epoch_mse**0.5:.2f} | \\\n","        Val. Loss: {valid_loss:.2f} | Val. MSE: {valid_mse:.2f} |  Val. RMSE: {valid_mse**0.5:.2f} |')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NzXeDgHmlDob"},"source":["# We evaluate performance on our dev set\n","def eval(data_iter, model):\n","    \"\"\"\n","    Evaluating model performance on the dev set\n","    \"\"\"\n","    model.eval()\n","    epoch_loss = 0\n","    epoch_sse = 0\n","    pred_all = []\n","    trg_all = []\n","    no_observations = 0\n","\n","    with torch.no_grad():\n","        for batch in data_iter:\n","            feature, target = batch\n","\n","            feature, target = feature.to(device), target.to(device)\n","\n","            # for RNN:\n","            model.batch_size = target.shape[0]\n","            no_observations = no_observations + target.shape[0]\n","            model.hidden = model.init_hidden()\n","\n","            predictions = model(feature).squeeze(1)\n","            loss = loss_fn(predictions, target)\n","\n","            # We get the mse\n","            pred, trg = predictions.detach().cpu().numpy(), target.detach().cpu().numpy()\n","            sse, __ = model_performance(pred, trg)\n","\n","            epoch_loss += loss.item()*target.shape[0]\n","            epoch_sse += sse\n","            pred_all.extend(pred)\n","            trg_all.extend(trg)\n","\n","    return epoch_loss/no_observations, epoch_sse/no_observations, np.array(pred_all), np.array(trg_all)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2_22fHHElDog"},"source":["# How we print the model performance\n","def model_performance(output, target, print_output=False):\n","    \"\"\"\n","    Returns SSE and MSE per batch (printing the MSE and the RMSE)\n","    \"\"\"\n","\n","    sq_error = (output - target)**2\n","\n","    sse = np.sum(sq_error)\n","    mse = np.mean(sq_error)\n","    rmse = np.sqrt(mse)\n","\n","    if print_output:\n","        print(f'| MSE: {mse:.2f} | RMSE: {rmse:.2f} |')\n","\n","    return sse, mse"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LcxmqrKhlDoj"},"source":["def create_vocab(data):\n","    \"\"\"\n","    Creating a corpus of all the tokens used\n","    \"\"\"\n","    tokenized_corpus = [] # Let us put the tokenized corpus in a list\n","\n","    for sentence in data:\n","\n","        tokenized_sentence = []\n","\n","        for token in sentence.split(' '): # simplest split is\n","\n","            tokenized_sentence.append(token)\n","\n","        tokenized_corpus.append(tokenized_sentence)\n","\n","    # Create single list of all vocabulary\n","    vocabulary = []  # Let us put all the tokens (mostly words) appearing in the vocabulary in a list\n","\n","    for sentence in tokenized_corpus:\n","\n","        for token in sentence:\n","\n","            if token not in vocabulary:\n","\n","                if True:\n","                    vocabulary.append(token)\n","\n","    return vocabulary, tokenized_corpus"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jzQ0KLXslDoq"},"source":["def collate_fn_padd(batch):\n","    '''\n","    We add padding to our minibatches and create tensors for our model\n","    '''\n","\n","    batch_labels = [l for f, l in batch]\n","    batch_features = [f for f, l in batch]\n","\n","    batch_features_len = [len(f) for f, l in batch]\n","\n","    seq_tensor = torch.zeros((len(batch), max(batch_features_len))).long()\n","\n","    for idx, (seq, seqlen) in enumerate(zip(batch_features, batch_features_len)):\n","        seq_tensor[idx, :seqlen] = torch.LongTensor(seq)\n","\n","    batch_labels = torch.FloatTensor(batch_labels)\n","\n","    return seq_tensor, batch_labels\n","\n","class Task1Dataset(Dataset):\n","\n","    def __init__(self, train_data, labels):\n","        self.x_train = train_data\n","        self.y_train = labels\n","\n","    def __len__(self):\n","        return len(self.y_train)\n","\n","    def __getitem__(self, item):\n","        return self.x_train[item], self.y_train[item]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rdeFaoc3lDpK"},"source":["#### Approach 2: No pre-trained representations"]},{"cell_type":"code","metadata":{"id":"46gm47T4lDpQ","outputId":"200c6ad7-aa53-4e25-8939-11b46e6e6759"},"source":["train_and_dev = train_df['edit']\n","\n","training_data, dev_data, training_y, dev_y = train_test_split(train_df['edit'], train_df['meanGrade'],\n","                                                                        test_size=(1-train_proportion),\n","                                                                        random_state=42)\n","\n","# We train a Tf-idf model\n","count_vect = CountVectorizer(stop_words='english')\n","train_counts = count_vect.fit_transform(training_data)\n","transformer = TfidfTransformer().fit(train_counts)\n","train_counts = transformer.transform(train_counts)\n","regression_model = LinearRegression().fit(train_counts, training_y)\n","\n","# Train predictions\n","predicted_train = regression_model.predict(train_counts)\n","\n","# Calculate Tf-idf using train and dev, and validate model on dev:\n","test_and_test_counts = count_vect.transform(train_and_dev)\n","transformer = TfidfTransformer().fit(test_and_test_counts)\n","\n","test_counts = count_vect.transform(dev_data)\n","\n","test_counts = transformer.transform(test_counts)\n","\n","# Dev predictions\n","predicted = regression_model.predict(test_counts)\n","\n","# We run the evaluation:\n","print(\"\\nTrain performance:\")\n","sse, mse = model_performance(predicted_train, training_y, True)\n","\n","print(\"\\nDev performance:\")\n","sse, mse = model_performance(predicted, dev_y, True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","Train performance:\n","| MSE: 0.13 | RMSE: 0.37 |\n","\n","Dev performance:\n","| MSE: 0.36 | RMSE: 0.60 |\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"9HyHwHkUlDpa"},"source":["#### Baseline for task 2"]},{"cell_type":"code","metadata":{"id":"7DA3q4o1lDpd","outputId":"fa88780f-7ed0-4c2b-f2cf-bc0b2883626d"},"source":["# Baseline for the task\n","pred_baseline = torch.zeros(len(dev_y)) + np.mean(training_y)\n","print(\"\\nBaseline performance:\")\n","sse, mse = model_performance(pred_baseline, dev_y, True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","Baseline performance:\n","| MSE: 0.34 | RMSE: 0.58 |\n"],"name":"stdout"}]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def get_tokenized_corpus(corpus):\n","  tokenized_corpus = []\n","\n","  for sentence in corpus:\n","    tokenized_sentence = []\n","    for token in sentence.split(' '): \n","      tokenized_sentence.append(token)\n","    tokenized_corpus.append(tokenized_sentence)\n"," \n","  return tokenized_corpus\n","\n","\n","def get_word2idx(tokenized_corpus):\n","  vocabulary = []\n","  for sentence in tokenized_corpus:\n","    for token in sentence:\n","        if token not in vocabulary:\n","            vocabulary.append(token)\n","  \n","  word2idx = {w: idx+1 for (idx, w) in enumerate(vocabulary)}\n","  word2idx['<pad>'] = 0\n","  \n","  return word2idx\n","\n","\n","def get_model_inputs(tokenized_corpus, word2idx, labels):\n","  vectorized_sents = [[word2idx[tok] for tok in sent if tok in word2idx] for sent in tokenized_corpus]\n","  sent_lengths = [len(sent) for sent in vectorized_sents]\n","  max_len = max(sent_lengths)\n","  sent_tensor = torch.zeros((len(vectorized_sents), max_len)).long()\n","  for idx, (sent, sentlen) in enumerate(zip(vectorized_sents, sent_lengths)):\n","    sent_tensor[idx, :sentlen] = torch.LongTensor(sent)\n","  label_tensor = torch.FloatTensor(labels)\n","  return sent_tensor, label_tensor"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["tokenized_corpus = get_tokenized_corpus(training_data)\n","word2idx = get_word2idx(tokenized_corpus)\n","train_sent_tensor, train_label_tensor = get_model_inputs(tokenized_corpus, word2idx, training_y)"]},{"cell_type":"code","metadata":{"id":"84nQDZyBlDpg"},"source":["class FFNN(nn.Module):\n","    def __init__(self, embedding_dim, hidden_dim, vocab_size, num_classes):  \n","        super(FFNN, self).__init__()\n","\n","        # padding_idx argument makes sure that the 0-th token in the vocabulary\n","        # is used for padding purposes i.e. its embedding will be a 0-vector\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n","        self.fc1 = nn.Linear(embedding_dim, hidden_dim)\n","        self.relu1 = nn.ReLU()\n","        self.fc2 = nn.Linear(hidden_dim, 1)  \n","    \n","    def forward(self, x):\n","        # x has shape (batch_size, max_sent_len)\n","\n","        embedded = self.embedding(x)\n","        sent_lens = x.ne(0).sum(1, keepdims=True)\n","        averaged = embedded.sum(1) / sent_lens\n","\n","        out = self.fc1(averaged)\n","        out = self.relu1(out)\n","        out = self.fc2(out)\n","        out = torch.clamp(out, min=0.0, max=3.0)\n","        return out"],"execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'nn' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-986172adad2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mFFNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFFNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;31m# padding_idx argument makes sure that the 0-th token in the vocabulary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"]}]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fix_seed()\n","EPOCHS = 10\n","LRATE = 0.5\n","EMBEDDING_DIM = 80\n","HIDDEN_DIM = 80\n","\n","model = FFNN(EMBEDDING_DIM, HIDDEN_DIM, len(word2idx))\n","print(model)\n","\n","optimizer = optim.SGD(model.parameters(), lr=LRATE)\n","loss_fn = nn.MSELoss()\n","\n","feature = train_sent_tensor\n","target = train_label_tensor\n","\n","print(f'Will train for {EPOCHS} epochs')\n","for epoch in range(1, EPOCHS + 1):\n","  # to ensure the dropout (explained later) is \"turned on\" while training\n","  # good practice to include even if do not use here\n","  model.train()\n","  \n","  # we zero the gradients as they are not removed automatically\n","  optimizer.zero_grad()\n","  \n","  # squeeze is needed as the predictions will have the shape (batch size, 1)\n","  # and we need to remove the dimension of size 1\n","  predictions = model(feature).squeeze(1)\n","\n","  # Compute the loss\n","  loss = loss_fn(predictions, target)\n","  train_loss = loss.item()\n","\n","  # calculate the gradient of each parameter\n","  loss.backward()\n","\n","  # update the parameters using the gradients and optimizer algorithm \n","  optimizer.step()\n","  \n","  print(f'| Epoch: {epoch:02} | Train Loss: {train_loss:.3f}')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}]}